/*
 * Copyright (c) 2010-2015 Advanced Micro Devices, Inc.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 * this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from this
 * software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * -- The probe filter is inclusive
 * -- If sharer information available, the sharer is probed
 * -- If sharer information not available, probes are broadcasted
 */

machine(MachineType:Directory, "AMD Baseline protocol")
: DirectoryMemory * directory;
  CacheMemory * L3CacheMemory;
  CacheMemory * ProbeFilterMemory; // 8 times the associativity
  Cycles response_latency := 5;
  Cycles l3_hit_latency := 50;
  bool noTCCdir := "False";
  bool CPUonly := "False";
  bool GPUonly := "False";
  int TCC_select_num_bits;
  bool useL3OnWT := "False";
  bool earlyDataToCore := "True"; //opt1
  bool noWBCleanVictimsToMem := "True"; //opt2 (bugfix)
  bool wbLLC := "True"; //opt3
  bool noWBCleanVictimsToLLC := "False"; //opt3_5
  bool ownerTracking := "True"; //opt4
  bool sharerTracking := "True"; //opt5 (enhancement on opt4)
  int num_sharers := 4;
  bool L2isWB;
  bool inclusiveDir := "False";
  Cycles to_memory_controller_latency := 1;

  // DMA
  MessageBuffer * requestFromDMA, network="From", virtual_network="1", vnet_type="request";
  MessageBuffer * responseToDMA, network="To", virtual_network="3", vnet_type="response";

  // From the Cores
  MessageBuffer * requestFromCores, network="From", virtual_network="0", vnet_type="request";
  MessageBuffer * responseFromCores, network="From", virtual_network="2", vnet_type="response";
  MessageBuffer * unblockFromCores, network="From", virtual_network="4", vnet_type="unblock";

  MessageBuffer * probeToCore, network="To", virtual_network="0", vnet_type="request";
  MessageBuffer * responseToCore, network="To", virtual_network="2", vnet_type="response";

  MessageBuffer * triggerQueue;
  MessageBuffer * L3triggerQueue;

  MessageBuffer * requestToMemory;
  MessageBuffer * responseFromMemory;
{
  // STATES
  state_declaration(State, desc="Directory states", default="Directory_State_U") {
    U, AccessPermission:Backing_Store,          desc="unblocked";
    BL, AccessPermission:Busy,                  desc="got L3 WB request";
    // BL is Busy because it's possible for the data only to be in the network
    // in the WB, L3 has sent it and gone on with its business in possibly I
    // state.
    BDR_M, AccessPermission:Backing_Store,      desc="DMA read, blocked waiting for memory";
    BDW_M, AccessPermission:Backing_Store,      desc="DMA write, blocked waiting for memory";
    BS_M, AccessPermission:Backing_Store,       desc="blocked waiting for memory";
    BM_M, AccessPermission:Backing_Store,       desc="blocked waiting for memory";
    B_M, AccessPermission:Backing_Store,        desc="blocked waiting for memory";
    BP, AccessPermission:Backing_Store,         desc="blocked waiting for probes, no need for memory";
    BDR_PM, AccessPermission:Backing_Store,     desc="DMA read, blocked waiting for probes and memory";
    BDW_PM, AccessPermission:Backing_Store,     desc="DMA write, blocked waiting for probes and memory";
    BS_PM, AccessPermission:Backing_Store,      desc="blocked waiting for probes and Memory";
    BM_PM, AccessPermission:Backing_Store,      desc="blocked waiting for probes and Memory";
    B_PM, AccessPermission:Backing_Store,       desc="blocked waiting for probes and Memory";
    BDR_Pm, AccessPermission:Backing_Store,     desc="DMA read, blocked waiting for probes, already got memory";
    BDW_Pm, AccessPermission:Backing_Store,     desc="DMA write, blocked waiting for probes, already got memory";
    BS_Pm, AccessPermission:Backing_Store,      desc="blocked waiting for probes, already got memory";
    BM_Pm, AccessPermission:Backing_Store,      desc="blocked waiting for probes, already got memory";
    B_Pm, AccessPermission:Backing_Store,       desc="blocked waiting for probes, already got memory";
    B, AccessPermission:Backing_Store,          desc="sent response, Blocked til ack";
    B_P, AccessPermission:Backing_Store,      desc="Back invalidation, waiting for probes";

    F, AccessPermission:Busy, desc="sent Flus, blocked till ack";
  }

  // Events
  enumeration(Event, desc="Directory events") {
    // CPU requests
    RdBlkS,             desc="...";
    RdBlkM,             desc="...";
    RdBlk,              desc="...";
    CtoD,               desc="...";
    WriteThrough,       desc="WriteThrough Message";
    Atomic,             desc="Atomic Message";

    // writebacks
    VicDirty,           desc="...";
    VicClean,           desc="...";
    CPUData,            desc="WB data from CPU";
    StaleWB,         desc="Notification that WB has been superceded by a probe";

    // probe responses
    CPUPrbResp,            desc="Probe Response Msg";

    ProbeAcksComplete,  desc="Probe Acks Complete";

    L3Hit,              desc="Hit in L3 return data to core";

    // Replacement
    PF_Repl,            desc="Replace address from probe filter";

    // Memory Controller
    MemData, desc="Fetched data from memory arrives";
    WBAck, desc="Writeback Ack from memory arrives";

    CoreUnblock,            desc="Core received data, unblock";
    UnblockWriteThrough,    desc="Unblock because of writethrough request finishing";

    StaleVicDirty,        desc="Core invalidated before VicDirty processed";

    // DMA
    DmaRead,            desc="DMA read";
    DmaWrite,           desc="DMA write";

    // Flush
    Flush,              desc="Flush entry";
  }

  enumeration(RequestType, desc="To communicate stats from transitions to recordStats") {
    L3DataArrayRead,    desc="Read the data array";
    L3DataArrayWrite,   desc="Write the data array";
    L3TagArrayRead,     desc="Read the data array";
    L3TagArrayWrite,    desc="Write the data array";

    PFTagArrayRead,     desc="Read the data array";
    PFTagArrayWrite,    desc="Write the data array";
  }

  // TYPES

  enumeration(ProbeFilterState, desc="") { // TODO should there be a default explicit?
    S,  desc="Shared by some/several L2s";
    O,  desc="Tracked / owned by some L2"; // in the L2s
    B,  desc="Blocked, This entry is being replaced";
    I,  desc="Not tracked / invalid";
  }

  // DirectoryEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry", main="false") {
    State DirectoryState,          desc="Directory state";
    NetDest VicDirtyIgnore,  desc="VicDirty coming from whom to ignore";
  }

  //TODO consider splitting this structure into CacheEntry and PFEntry
  structure(CacheEntry, desc="...", interface="AbstractCacheEntry") {
    DataBlock DataBlk,          desc="data for the block";
    MachineID LastSender,       desc="Mach which this block came from";
    bool Dirty,                 desc="dirty bit to alleviate excessive clean-victim writebacks";
    ProbeFilterState pfState,   desc="ProbeFilter state",default="Directory_ProbeFilterState_I";
    MachineID Owner,            desc="Owner machineID -- for restricting the probe broadcast to unicast";
    NetDest Sharers,            desc="Sharers of the line which need to be probed";
  }

  structure(TBE, desc="...") {
    State TBEState,     desc="Transient state";
    DataBlock DataBlk,  desc="data for the block";
    bool Dirty,         desc="Is the data dirty?";
    int NumPendingAcks,        desc="num acks expected";
    MachineID OriginalRequestor,        desc="Original Requestor";
    MachineID CURequestor,        desc="CU that initiated the request";
    bool Cached,        desc="data hit in Cache";
    bool MemData,       desc="Got MemData?",default="false";
    bool wtData,       desc="Got write through data?",default="false";
    bool readyToResp,       desc="Got dirty probe with data",default="false";
    bool responded,       desc="Got dirty probe with data",default="false";
    bool atomicData,   desc="Got Atomic op?",default="false";
    // Note, protocol invariant: atomicData = atomicDataReturn || atomicDataNoReturn;
    bool atomicDataReturn, desc="Got Atomic op and need return value?",default="false";
    bool atomicDataNoReturn, desc="Got Atomic op and don't need return value?",default="false";
    Cycles InitialRequestTime, desc="...";
    Cycles ForwardRequestTime, desc="...";
    Cycles ProbeRequestStartTime, desc="...";
    MachineID LastSender, desc="Mach which this block came from";
    bool L3Hit, default="false", desc="Was this an L3 hit?";
    uint64_t probe_id,        desc="probe id for lifetime profiling";
    WriteMask writeMask,    desc="outstanding write through mask";
    int Len,            desc="Length of memory request for DMA";
    // GLC is passed along because it is needed in the return path
    bool isGLCSet,      desc="Bypass GPU L1 Cache";
    bool isSLCSet,      desc="Bypass GPU L1 and L2 Cache";
    Addr demandAddress,  desc="Address of demand request which caused probe filter eviction";
    bool cupfOn, desc="if you need to update PF conditionally (see cupf)";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Addr);
    void allocate(Addr);
    void deallocate(Addr);
    bool isPresent(Addr);
  }

  TBETable TBEs, template="<Directory_TBE>", constructor="m_number_of_TBEs";

  int TCC_select_low_bit, default="RubySystem::getBlockSizeBits()";

  Tick clockEdge();
  Tick cyclesToTicks(Cycles c);

  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpAllBuffers();
  void wakeUpAllBuffers(Addr a);
  void wakeUpBuffers(Addr a);
  Cycles curCycle();

  Entry getDirectoryEntry(Addr addr), return_by_pointer="yes" {
    Entry dir_entry := static_cast(Entry, "pointer", directory.lookup(addr));

    if (is_valid(dir_entry)) {
      return dir_entry;
    }

    dir_entry :=  static_cast(Entry, "pointer",
                              directory.allocate(addr, new Entry));
    return dir_entry;
  }

  State getState(TBE tbe, CacheEntry entry, Addr addr) {
    return getDirectoryEntry(addr).DirectoryState;
  }

  void setState(TBE tbe, CacheEntry entry, Addr addr, State state) {
    getDirectoryEntry(addr).DirectoryState := state;
  }

  void functionalRead(Addr addr, Packet *pkt) {
    DPRINTF(OPT1, "FunctionalRead at addr: 0x%x", addr);
    TBE tbe := TBEs.lookup(addr);
    if(is_valid(tbe)) {
      testAndRead(addr, tbe.DataBlk, pkt);
    } else {
      functionalMemoryRead(pkt);
    }
  }

  int functionalWrite(Addr addr, Packet *pkt) {
    DPRINTF(OPT1, "FunctionalWrite at addr: 0x%x", addr);
    int num_functional_writes := 0;

    TBE tbe := TBEs.lookup(addr);
    if(is_valid(tbe)) {
      num_functional_writes := num_functional_writes +
            testAndWrite(addr, tbe.DataBlk, pkt);
    }

    num_functional_writes := num_functional_writes
        + functionalMemoryWrite(pkt);
    return num_functional_writes;
  }

  AccessPermission getAccessPermission(Addr addr) {
    // For this Directory, all permissions are just tracked in Directory, since
    // it's not possible to have something in TBE but not Dir, just keep track
    // of state all in one place.
    if (directory.isPresent(addr)) {
      return Directory_State_to_permission(getDirectoryEntry(addr).DirectoryState);
    }

    return AccessPermission:NotPresent;
  }

  void setAccessPermission(CacheEntry entry, Addr addr, State state) {
    getDirectoryEntry(addr).changePermission(Directory_State_to_permission(state));
  }

  void recordRequestType(RequestType request_type, Addr addr) {
    if (request_type == RequestType:L3DataArrayRead) {
      L3CacheMemory.recordRequestType(CacheRequestType:DataArrayRead, addr);
    } else if (request_type == RequestType:L3DataArrayWrite) {
      L3CacheMemory.recordRequestType(CacheRequestType:DataArrayWrite, addr);
    } else if (request_type == RequestType:L3TagArrayRead) {
      L3CacheMemory.recordRequestType(CacheRequestType:TagArrayRead, addr);
    } else if (request_type == RequestType:L3TagArrayWrite) {
      L3CacheMemory.recordRequestType(CacheRequestType:TagArrayWrite, addr);
    } else if (request_type == RequestType:PFTagArrayRead) {
      ProbeFilterMemory.recordRequestType(CacheRequestType:TagArrayRead, addr);
    } else if (request_type == RequestType:PFTagArrayWrite) {
      ProbeFilterMemory.recordRequestType(CacheRequestType:TagArrayWrite, addr);
    }
  }

  bool checkResourceAvailable(RequestType request_type, Addr addr) {
    if (request_type == RequestType:L3DataArrayRead) {
      return L3CacheMemory.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (request_type == RequestType:L3DataArrayWrite) {
      return L3CacheMemory.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (request_type == RequestType:L3TagArrayRead) {
      return L3CacheMemory.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else if (request_type == RequestType:L3TagArrayWrite) {
      return L3CacheMemory.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else if (request_type == RequestType:PFTagArrayRead) {
      return ProbeFilterMemory.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else if (request_type == RequestType:PFTagArrayWrite) {
      return ProbeFilterMemory.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else {
      error("Invalid RequestType type in checkResourceAvailable");
      return true;
    }
  }

  // ** OUT_PORTS **
  out_port(dmaResponseQueue_out, DMAResponseMsg, responseToDMA);

  bool isNotPresentProbeFilter(Addr address) {
    if (ProbeFilterMemory.isTagPresent(address) ||
        ProbeFilterMemory.cacheAvail(address)) {
        return false;
    }
    return true;
  }

  // ** OUT_PORTS **
  out_port(probeNetwork_out, NBProbeRequestMsg, probeToCore);
  out_port(responseNetwork_out, ResponseMsg, responseToCore);

  out_port(triggerQueue_out, TriggerMsg, triggerQueue);
  out_port(L3TriggerQueue_out, TriggerMsg, L3triggerQueue);

  out_port(memQueue_out, MemoryMsg, requestToMemory);

  // ** IN_PORTS **

  // DMA Ports
  in_port(dmaRequestQueue_in, DMARequestMsg, requestFromDMA, rank=6) {
    if (dmaRequestQueue_in.isReady(clockEdge())) {
      peek(dmaRequestQueue_in, DMARequestMsg) {
        TBE tbe := TBEs.lookup(in_msg.LineAddress);
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(in_msg.LineAddress));
        if (in_msg.Type == DMARequestType:READ) {
          trigger(Event:DmaRead, in_msg.LineAddress, entry, tbe);
        } else if (in_msg.Type == DMARequestType:WRITE) {
          trigger(Event:DmaWrite, in_msg.LineAddress, entry, tbe);
        } else {
          error("Unknown DMA msg");
        }
      }
    }
  }

  // Trigger Queue
  in_port(triggerQueue_in, TriggerMsg, triggerQueue, rank=5) {
    if (triggerQueue_in.isReady(clockEdge())) {
      peek(triggerQueue_in, TriggerMsg) {
        TBE tbe := TBEs.lookup(in_msg.addr);
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(in_msg.addr));
        if (in_msg.Type == TriggerType:AcksComplete) {
          trigger(Event:ProbeAcksComplete, in_msg.addr, entry, tbe);
        }else if (in_msg.Type == TriggerType:UnblockWriteThrough) {
          trigger(Event:UnblockWriteThrough, in_msg.addr, entry, tbe);
        } else {
          error("Unknown trigger msg");
        }
      }
    }
  }

  in_port(L3TriggerQueue_in, TriggerMsg, L3triggerQueue, rank=4) {
    if (L3TriggerQueue_in.isReady(clockEdge())) {
      peek(L3TriggerQueue_in, TriggerMsg) {
        TBE tbe := TBEs.lookup(in_msg.addr);
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(in_msg.addr));
        if (in_msg.Type == TriggerType:L3Hit) {
          trigger(Event:L3Hit, in_msg.addr, entry, tbe);
        } else {
          error("Unknown trigger msg");
        }
      }
    }
  }

  // Unblock Network
  in_port(unblockNetwork_in, UnblockMsg, unblockFromCores, rank=3) {
    if (unblockNetwork_in.isReady(clockEdge())) {
      peek(unblockNetwork_in, UnblockMsg) {
        TBE tbe := TBEs.lookup(in_msg.addr);
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(in_msg.addr));
        trigger(Event:CoreUnblock, in_msg.addr, entry, tbe);
      }
    }
  }

  // Core response network
  in_port(responseNetwork_in, ResponseMsg, responseFromCores, rank=2) {
    if (responseNetwork_in.isReady(clockEdge())) {
      peek(responseNetwork_in, ResponseMsg) {
        TBE tbe := TBEs.lookup(in_msg.addr);
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(in_msg.addr));
        if (in_msg.Type == CoherenceResponseType:CPUPrbResp) {
          trigger(Event:CPUPrbResp, in_msg.addr, entry, tbe);
        } else if (in_msg.Type == CoherenceResponseType:CPUData) {
          trigger(Event:CPUData, in_msg.addr, entry, tbe);
        } else if (in_msg.Type == CoherenceResponseType:StaleNotif) {
            trigger(Event:StaleWB, in_msg.addr, entry, tbe);
        } else {
          error("Unexpected response type");
        }
      }
    }
  }

  // off-chip memory request/response is done
  in_port(memQueue_in, MemoryMsg, responseFromMemory, rank=1) {
    if (memQueue_in.isReady(clockEdge())) {
      peek(memQueue_in, MemoryMsg) {
        TBE tbe := TBEs.lookup(in_msg.addr);
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(in_msg.addr));
        if (in_msg.Type == MemoryRequestType:MEMORY_READ) {
          trigger(Event:MemData, in_msg.addr, entry, tbe);
          DPRINTF(RubySlicc, "%s\n", in_msg);
        } else if (in_msg.Type == MemoryRequestType:MEMORY_WB) {
          trigger(Event:WBAck, in_msg.addr, entry, tbe); // ignore WBAcks, don't care about them.
        } else {
          DPRINTF(RubySlicc, "%s\n", in_msg.Type);
          error("Invalid message");
        }
      }
    }
  }

  in_port(requestNetwork_in, CPURequestMsg, requestFromCores, rank=0) {
    if (requestNetwork_in.isReady(clockEdge())) {
      peek(requestNetwork_in, CPURequestMsg) {
        TBE tbe := TBEs.lookup(in_msg.addr);
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(in_msg.addr));
        
        // inclusive directory, so cannot not have line tracked when ownerTracking
        if (ownerTracking || sharerTracking) {
          if (in_msg.Type == CoherenceRequestType:VicClean || in_msg.Type == CoherenceRequestType:VicDirty) {
            DPRINTF(OPT5, "SHL: VicDirty/VicClean received [0x%x]\n", in_msg.addr);
            assert (!isNotPresentProbeFilter(in_msg.addr));
          }
        }

        if ((ownerTracking || sharerTracking) && isNotPresentProbeFilter(in_msg.addr)) { 
          // if neither tag present nor free line available
          DPRINTF(OPT4, "PF full, victimizing for 0x%x\n", in_msg.addr);
          Addr victim := ProbeFilterMemory.cacheProbe(in_msg.addr);
          DPRINTF(OPT4, "PF full, victimizing 0x%x\n", victim);
          tbe := TBEs.lookup(victim);
          entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(victim));
          trigger(Event:PF_Repl, victim, entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:RdBlk) {
          trigger(Event:RdBlk, in_msg.addr, entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:RdBlkS) {
          trigger(Event:RdBlkS, in_msg.addr, entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:RdBlkM) {
          trigger(Event:RdBlkM, in_msg.addr, entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:WriteThrough) {
          trigger(Event:WriteThrough, in_msg.addr, entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:Atomic  ||
                   in_msg.Type == CoherenceRequestType:AtomicReturn ||
                   in_msg.Type == CoherenceRequestType:AtomicNoReturn) {
          trigger(Event:Atomic, in_msg.addr, entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:VicDirty) {
          if (getDirectoryEntry(in_msg.addr).VicDirtyIgnore.isElement(in_msg.Requestor)) {
            DPRINTF(RubySlicc, "Dropping VicDirty for address 0x%x\n", in_msg.addr);
            trigger(Event:StaleVicDirty, in_msg.addr, entry, tbe);
          } else {
            DPRINTF(RubySlicc, "Got VicDirty from %s on 0x%x\n", in_msg.Requestor, in_msg.addr);
            trigger(Event:VicDirty, in_msg.addr, entry, tbe);
          }
        } else if (in_msg.Type == CoherenceRequestType:VicClean) {
          if (getDirectoryEntry(in_msg.addr).VicDirtyIgnore.isElement(in_msg.Requestor)) {
            DPRINTF(RubySlicc, "Dropping VicClean for address 0x%x\n", in_msg.addr);
            trigger(Event:StaleVicDirty, in_msg.addr, entry, tbe);
          } else {
            DPRINTF(RubySlicc, "Got VicClean from %s on 0x%x\n", in_msg.Requestor, in_msg.addr);
            trigger(Event:VicClean, in_msg.addr, entry, tbe);
          }
        } else if (in_msg.Type == CoherenceRequestType:WriteFlush) {
            DPRINTF(RubySlicc, "Got Flush from %s on 0x%x\n", in_msg.Requestor, in_msg.addr);
            trigger(Event:Flush, in_msg.addr, entry, tbe);
        } else {
          error("Bad request message type");
        }
      }
    }
  }

  // Actions
  action(dd_sendResponseDmaData, "dd", desc="send DMA data response") {
    enqueue(dmaResponseQueue_out, DMAResponseMsg, response_latency) {
      out_msg.LineAddress := address;
      out_msg.Type := DMAResponseType:DATA;
      out_msg.Destination.add(tbe.OriginalRequestor);
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(da_sendResponseDmaAck, "da", desc="send DMA data response") {
    enqueue(dmaResponseQueue_out, DMAResponseMsg, response_latency) {
      out_msg.LineAddress := address;
      out_msg.Type := DMAResponseType:ACK;
      out_msg.Destination.add(tbe.OriginalRequestor);
      out_msg.MessageSize := MessageSizeType:Response_Control;
    }
  }

  action(s_sendResponseS, "s", desc="send Shared response") {
    if(earlyDataToCore && !tbe.responded || !earlyDataToCore) {
      DPRINTF(OPT1, "NotEarly: Responding to RdBlkS\n");
      enqueue(responseNetwork_out, ResponseMsg, response_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:NBSysResp;
        if (tbe.L3Hit) {
          out_msg.Sender := createMachineID(MachineType:L3Cache, intToID(0));
        } else {
          out_msg.Sender := machineID;
        }
        out_msg.Destination.add(tbe.OriginalRequestor);
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        out_msg.Dirty := false; //ensured by the probe-type (downgrade)
        out_msg.State := CoherenceState:Shared;
        out_msg.InitialRequestTime := tbe.InitialRequestTime;
        out_msg.ForwardRequestTime := tbe.ForwardRequestTime;
        out_msg.ProbeRequestStartTime := tbe.ProbeRequestStartTime;
        out_msg.OriginalResponder := tbe.LastSender;
        out_msg.CURequestor := tbe.CURequestor;
        out_msg.L3Hit := tbe.L3Hit; // not used in the destination
        DPRINTF(RubySlicc, "%s\n", out_msg);
      }
    }
  }

  action(es_sendResponseES, "es", desc="send Exclusive or Shared response") {
    if(earlyDataToCore && !tbe.responded || !earlyDataToCore) {
      DPRINTF(OPT1, "NotEarly: Responding to RdBlk\n");
      enqueue(responseNetwork_out, ResponseMsg, response_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:NBSysResp;
        if (tbe.L3Hit) {
          out_msg.Sender := createMachineID(MachineType:L3Cache, intToID(0));
        } else {
          out_msg.Sender := machineID;
        }
        out_msg.Destination.add(tbe.OriginalRequestor);
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        out_msg.Dirty := tbe.Dirty;
        // this is ignored by GPU
        // this is also dependant on how many probe acknowledgements intimate hit at probe dest
        if (tbe.Cached) {
          out_msg.State := CoherenceState:Shared;
        } else {
          out_msg.State := CoherenceState:Exclusive;
        }
        out_msg.InitialRequestTime := tbe.InitialRequestTime;
        out_msg.ForwardRequestTime := tbe.ForwardRequestTime;
        out_msg.ProbeRequestStartTime := tbe.ProbeRequestStartTime;
        out_msg.OriginalResponder := tbe.LastSender;
        out_msg.L3Hit := tbe.L3Hit; // unused in destination
        out_msg.isGLCSet := tbe.isGLCSet;
        out_msg.isSLCSet := tbe.isSLCSet;
        out_msg.CURequestor := tbe.CURequestor;
        DPRINTF(RubySlicc, "%s\n", out_msg);
      }
    }
  }

  action(m_sendResponseM, "m", desc="send Modified response") {
    //if(earlyDataToCore && !tbe.responded || !earlyDataToCore) {
    //  DPRINTF(OPT1, "NotEarly: Responding to RdBlkM\n");
      if (tbe.wtData) {
        enqueue(triggerQueue_out, TriggerMsg, 1) {
          out_msg.addr := address;
          out_msg.Type := TriggerType:UnblockWriteThrough;
        }
      } else {
        enqueue(responseNetwork_out, ResponseMsg, response_latency) {
          out_msg.addr := address;
          out_msg.Type := CoherenceResponseType:NBSysResp;
          if (tbe.L3Hit) {
            out_msg.Sender := createMachineID(MachineType:L3Cache, intToID(0));
          } else {
            out_msg.Sender := machineID;
          }
          out_msg.Destination.add(tbe.OriginalRequestor);
          out_msg.DataBlk := tbe.DataBlk;
          out_msg.MessageSize := MessageSizeType:Response_Data;
          out_msg.Dirty := tbe.Dirty;
          out_msg.State := CoherenceState:Modified;
          out_msg.CtoD := false;
          out_msg.InitialRequestTime := tbe.InitialRequestTime;
          out_msg.ForwardRequestTime := tbe.ForwardRequestTime;
          out_msg.ProbeRequestStartTime := tbe.ProbeRequestStartTime;
          out_msg.OriginalResponder := tbe.LastSender;
          out_msg.isGLCSet := tbe.isGLCSet;
          out_msg.isSLCSet := tbe.isSLCSet;
          if(tbe.atomicData){
            out_msg.CURequestor := tbe.CURequestor;
          }
          out_msg.L3Hit := tbe.L3Hit; // unused in destination
          DPRINTF(RubySlicc, "%s\n", out_msg);
        }
        if (tbe.atomicData) {
          enqueue(triggerQueue_out, TriggerMsg, 1) {
            out_msg.addr := address;
            out_msg.Type := TriggerType:UnblockWriteThrough;
          }
        }
      }
    //}
  }

  // this action is not actively used currently
  action(c_sendResponseCtoD, "c", desc="send CtoD Ack") {
    enqueue(responseNetwork_out, ResponseMsg, response_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:NBSysResp;
      out_msg.Sender := machineID;
      out_msg.Destination.add(tbe.OriginalRequestor);
      out_msg.MessageSize := MessageSizeType:Response_Control;
      out_msg.Dirty := false;
      out_msg.State := CoherenceState:Modified;
      out_msg.CtoD := true;
      out_msg.InitialRequestTime := tbe.InitialRequestTime;
      out_msg.ForwardRequestTime := curCycle();
      out_msg.ProbeRequestStartTime := tbe.ProbeRequestStartTime;
      out_msg.isGLCSet := tbe.isGLCSet;
      out_msg.isSLCSet := tbe.isSLCSet;
      DPRINTF(RubySlicc, "%s\n", out_msg);
    }
  }

  action(w_sendResponseWBAck, "w", desc="send WB Ack") {
    peek(requestNetwork_in, CPURequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, 1) {
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:NBSysWBAck;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.CURequestor := in_msg.CURequestor;
        out_msg.Sender := machineID;
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
        out_msg.InitialRequestTime := in_msg.InitialRequestTime;
        out_msg.ForwardRequestTime := curCycle();
        out_msg.ProbeRequestStartTime := curCycle();
        out_msg.instSeqNum := in_msg.instSeqNum;
        out_msg.isGLCSet := in_msg.isGLCSet;
        out_msg.isSLCSet := in_msg.isSLCSet;
      }
    }
  }

  action(rf_sendResponseFlush, "rf", desc="send Flush Ack") {
    peek(memQueue_in, MemoryMsg) {
      enqueue(responseNetwork_out, ResponseMsg, 1) {
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:NBSysWBAck;
        out_msg.Destination.add(tbe.OriginalRequestor);
        out_msg.CURequestor := tbe.CURequestor;
        out_msg.Sender := machineID;
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
        out_msg.InitialRequestTime := tbe.InitialRequestTime;
        out_msg.ForwardRequestTime := curCycle();
        out_msg.ProbeRequestStartTime := curCycle();
        //out_msg.instSeqNum := in_msg.instSeqNum;
      }
    }
  }

  action(l_queueMemWBReq, "lq", desc="Write WB data to memory") {
    peek(responseNetwork_in, ResponseMsg) {
      enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
        out_msg.addr := address;
        out_msg.Type := MemoryRequestType:MEMORY_WB;
        out_msg.Sender := machineID;
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
        out_msg.DataBlk := in_msg.DataBlk;
      }
    }
  }

  action(qdr_queueDmaRdReq, "qdr", desc="Read data from memory for DMA") {
    peek(dmaRequestQueue_in, DMARequestMsg) {
      if (L3CacheMemory.isTagPresent(address)) {
        enqueue(L3TriggerQueue_out, TriggerMsg, l3_hit_latency) {
          out_msg.addr := address;
          out_msg.Type := TriggerType:L3Hit;
        }
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(address));

        // tbe.DataBlk may have partial data (e.g., for DMA writes). Make sure
        // not to clobber the data before merging it with the L3 cache data.
        DataBlock tmpBlk := entry.DataBlk;
        tmpBlk.copyPartial(tbe.DataBlk, tbe.writeMask);
        tbe.DataBlk := tmpBlk;

        tbe.L3Hit := true;
        tbe.MemData := true;
        L3CacheMemory.deallocate(address); // this is done to ensure exclusivity of L3
      } else {
        enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
          out_msg.addr := address;
          out_msg.Type := MemoryRequestType:MEMORY_READ;
          out_msg.Sender := machineID;
          out_msg.MessageSize := MessageSizeType:Request_Control;
        }
      }
    }
  }

  action(l_queueMemRdReq, "lr", desc="Read data from memory") {
    peek(requestNetwork_in, CPURequestMsg) {
      // TODO Below is an optimization, not functionally necessary;
      //bool readMem := false;
      //if(ownerTracking) {
      //  assert(ProbeFilterMemory.isTagPresent(address));
      //  CacheEntry pf := static_cast(CacheEntry,
      //      "pointer", ProbeFilterMemory.lookup(address));
      //  readMem := (pf.pfState == ProbeFilterState:I 
      //              || pf.pfState == ProbeFilterState:S && (tbe.TBEState == State:BS_PM || (tbe.TBEState == State:B_PM) && machineIDToMachineType(in_msg.Requestor) == MachineType:CorePair)
      //              );
      //  DPRINTF(OPT4, "Line status in PF: 0x%x:  %s", address, pf.pfState);
      //}
      //if (
      //    !ownerTracking || 
      //    ownerTracking && readMem
      //   ) {
        if (L3CacheMemory.isTagPresent(address)) {
          enqueue(L3TriggerQueue_out, TriggerMsg, l3_hit_latency) {
            out_msg.addr := address;
            out_msg.Type := TriggerType:L3Hit;
            DPRINTF(RubySlicc, "%s\n", out_msg);
          }
          CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(address));
          if (tbe.Dirty == false) {
            tbe.DataBlk := entry.DataBlk;
          }
          tbe.LastSender := entry.LastSender;
          tbe.L3Hit := true;
          tbe.MemData := true;
          L3CacheMemory.deallocate(address); // to maintain exclusivity TODO -- doesn't make sense
        } else {
          enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
            out_msg.addr := address;
            out_msg.Type := MemoryRequestType:MEMORY_READ;
            out_msg.Sender := machineID;
            out_msg.MessageSize := MessageSizeType:Request_Control;
          }
        }
      //} else if (ownerTracking && !readMem) {
      //  enqueue(L3TriggerQueue_out, TriggerMsg, l3_hit_latency) {
      //    out_msg.addr := address;
      //    out_msg.Type := TriggerType:L3Hit;
      //    DPRINTF(RubySlicc, "%s\n", out_msg);
      //    // TODO something messes up here!
      //  }
      //}
    }
  }

  //This action profiles a hit or miss for a given request or write back.
  //It should be called after l_queueMemRdReq, qdr_queueDmaRdReq, and al_allocateL3Block
  //actions (where the tag has been checked and the L3Hit Flag is set) and before the TBE is
  //deallocated in dt_deallocateTBE (only for WB) as it checks the L3Hit flag of the TBE entry.
  action(pr_profileL3HitMiss, "pr_l3hm", desc="L3 Hit or Miss Profile") {
    if (tbe.L3Hit) {
      L3CacheMemory.profileDemandHit();
    } else {
      L3CacheMemory.profileDemandMiss();
    }
  }

  action(icd_probeInvCoreDataForDMA, "icd", desc="Probe inv cores, return data for DMA") {
    peek(dmaRequestQueue_in, DMARequestMsg) {
      DPRINTF(OPT4, "DMA stuff really happens!\n");
      NetDest probe_dests;
      // Add relevant machine types based on CPUonly, GPUonly and noTCCdir:
      //  CPUonly &&  GPUonly -> Invalid
      //  CPUonly && !GPUonly -> Add CorePairs
      // !CPUonly &&  GPUonly -> Add TCCs or TCC dirs
      // !CPUonly && !GPUonly -> Add CorePairs and TCCs or TCC dirs
      if (CPUonly) {
        assert(!GPUonly);
        probe_dests.broadcast(MachineType:CorePair);

      } else {
        // CPU + GPU system
        if (!GPUonly) {
          probe_dests.broadcast(MachineType:CorePair);
        }

        // CPU + GPU or GPU only system
        if (noTCCdir) {
          probe_dests.add(mapAddressToRange(address, MachineType:TCC,
                                            TCC_select_low_bit,
                                            TCC_select_num_bits));
        } else {
          probe_dests.add(mapAddressToRange(address, MachineType:TCCdir,
                                            TCC_select_low_bit,
                                            TCC_select_num_bits));
        }
      }
      probe_dests.remove(in_msg.Requestor);

      if (probe_dests.count() > 0) {
        enqueue(probeNetwork_out, NBProbeRequestMsg, response_latency) {
          out_msg.addr := address;
          out_msg.Type := ProbeRequestType:PrbInv;
          out_msg.ReturnData := true;
          out_msg.MessageSize := MessageSizeType:Control;
          out_msg.Destination := probe_dests;
          tbe.NumPendingAcks := out_msg.Destination.count();
          DPRINTF(RubySlicc, "%s\n", out_msg);
          APPEND_TRANSITION_COMMENT(" dc: Acks remaining: ");
          APPEND_TRANSITION_COMMENT(tbe.NumPendingAcks);
          tbe.ProbeRequestStartTime := curCycle();
          assert(out_msg.Destination.count() > 0);
        }
      }

      if (probe_dests.count() == 0) {
        enqueue(triggerQueue_out, TriggerMsg, 1) {
          out_msg.addr := address;
          out_msg.Type := TriggerType:AcksComplete;
        }
      }
    }
  }

  action(dc_probeInvCoreData, "dc", desc="probe inv cores, return data") {
    peek(requestNetwork_in, CPURequestMsg) {
      NetDest probe_dests;
      bool noProbe := false;
      if(ownerTracking || sharerTracking) {
        CacheEntry entry := static_cast(CacheEntry, 
            "pointer", ProbeFilterMemory.lookup(address));
        noProbe := entry.pfState == ProbeFilterState:I;
        if (sharerTracking && !noProbe) {
          probe_dests := entry.Sharers;
          // sharers list does not include owner
          if(entry.pfState == ProbeFilterState:O) {
            probe_dests.add(entry.Owner);
          }
          DPRINTF(OPT5, "SHL [0x%x] Caches to be invalidated: %s\n", address, probe_dests);
        }
      }

      // Add relevant machine types based on CPUonly, GPUonly and noTCCdir:
      //  CPUonly &&  GPUonly -> Invalid
      //  CPUonly && !GPUonly -> Add CorePairs
      // !CPUonly &&  GPUonly -> Add TCCs or TCC dirs if no write conflict
      // !CPUonly && !GPUonly -> Add CorePairs and TCCs or TCC dirs
      //                         if no write conflict
      if (CPUonly) {
        assert(!GPUonly);
        if(!sharerTracking) {
          if(!ownerTracking || ownerTracking && !noProbe) {
            probe_dests.broadcast(MachineType:CorePair);
          } else {
            DPRINTF(OPT4, "Skipping probing CorePair because it does not have line\n");
          }
        } // else: sharer tracking already has updated probe_dests above
      } else {
        // CPU + GPU system
        if (!GPUonly) {
          if (!sharerTracking) {
            if(!ownerTracking || ownerTracking && !noProbe) {
              probe_dests.broadcast(MachineType:CorePair);
            } else {
              DPRINTF(OPT4, "Skipping probing CorePair because it does not have line\n");
            }
          }
        }

        // CPU + GPU or GPU only system
        // conservative; might be able to elide probes here as well
        if ((in_msg.Type != CoherenceRequestType:WriteThrough &&
             in_msg.Type != CoherenceRequestType:Atomic &&
             in_msg.Type != CoherenceRequestType:AtomicReturn &&
             in_msg.Type != CoherenceRequestType:AtomicNoReturn) ||
             !in_msg.NoWriteConflict) {
          if (noTCCdir) {
            //if(!ownerTracking || ownerTracking && !noProbe) {
              if(!probe_dests.isElement(mapAddressToRange(address, MachineType:TCC,
                                              TCC_select_low_bit,
                                              TCC_select_num_bits))) {
                probe_dests.add(mapAddressToRange(address, MachineType:TCC,
                                              TCC_select_low_bit,
                                              TCC_select_num_bits));
              }
            //} else {
            //  DPRINTF(OPT4, "Skipping probing TCC because it does not have line\n");
            //}
          } else {
            //if(!ownerTracking || ownerTracking && !noProbe) {
              if(!probe_dests.isElement(mapAddressToRange(address, MachineType:TCCdir,
                                              TCC_select_low_bit,
                                              TCC_select_num_bits))) {
                probe_dests.add(mapAddressToRange(address, MachineType:TCCdir,
                                              TCC_select_low_bit,
                                              TCC_select_num_bits));
              }
            //} else {
            //  DPRINTF(OPT4, "Skipping probing TCC because it does not have line\n");
            //}
          }
        }
      }
      probe_dests.remove(in_msg.Requestor);

      if (probe_dests.count() > 0) {
        enqueue(probeNetwork_out, NBProbeRequestMsg, response_latency) {
          out_msg.addr := address;
          out_msg.Type := ProbeRequestType:PrbInv;
          out_msg.ReturnData := true;
          out_msg.MessageSize := MessageSizeType:Control;
          out_msg.Destination := probe_dests;
          tbe.NumPendingAcks := out_msg.Destination.count();
          out_msg.isGLCSet := in_msg.isGLCSet;
          out_msg.isSLCSet := in_msg.isSLCSet;
          DPRINTF(RubySlicc, "%s\n", out_msg);
          APPEND_TRANSITION_COMMENT(" dc: Acks remaining: ");
          APPEND_TRANSITION_COMMENT(tbe.NumPendingAcks);
          tbe.ProbeRequestStartTime := curCycle();
          assert(out_msg.Destination.count() > 0);
        }
      }

      if (probe_dests.count() == 0) {
        enqueue(triggerQueue_out, TriggerMsg, 1) {
          out_msg.addr := address;
          out_msg.Type := TriggerType:AcksComplete;
        }
      }
    }
  }

  action(scd_probeShrCoreDataForDma, "dsc", desc="probe shared cores, return data for DMA") {
    peek(dmaRequestQueue_in, DMARequestMsg) {
      DPRINTF(OPT4, "DMA stuff really happens!\n");
      NetDest probe_dests;
      // Add relevant machine types based on CPUonly, GPUonly and noTCCdir:
      //  CPUonly &&  GPUonly -> Invalid
      //  CPUonly && !GPUonly -> Add CorePairs
      // !CPUonly &&  GPUonly -> Add TCCs or TCC dirs
      // !CPUonly && !GPUonly -> Add CorePairs and TCCs or TCC dirs
      if (CPUonly) {
        assert(!GPUonly);
        probe_dests.broadcast(MachineType:CorePair);
      } else {
        // CPU + GPU system
        if (!GPUonly) {
          probe_dests.broadcast(MachineType:CorePair);
        }

        // CPU + GPU or GPU only system
        // We don't need to notify TCC about reads
        if (!noTCCdir) {
          probe_dests.add(mapAddressToRange(address, MachineType:TCCdir,
                                            TCC_select_low_bit,
                                            TCC_select_num_bits));
        }
      }
      probe_dests.remove(in_msg.Requestor);

      if (probe_dests.count() > 0) {
        enqueue(probeNetwork_out, NBProbeRequestMsg, response_latency) {
          out_msg.addr := address;
          out_msg.Type := ProbeRequestType:PrbDowngrade;
          out_msg.ReturnData := true;
          out_msg.MessageSize := MessageSizeType:Control;
          out_msg.Destination := probe_dests;
          tbe.NumPendingAcks := out_msg.Destination.count();
          DPRINTF(RubySlicc, "%s\n", (out_msg));
          APPEND_TRANSITION_COMMENT(" sc: Acks remaining: ");
          APPEND_TRANSITION_COMMENT(tbe.NumPendingAcks);
          tbe.ProbeRequestStartTime := curCycle();
          assert(out_msg.Destination.count() > 0);
        }
      }

      if (probe_dests.count() == 0) {
        enqueue(triggerQueue_out, TriggerMsg, 1) {
          out_msg.addr := address;
          out_msg.Type := TriggerType:AcksComplete;
        }
      }
    }
  }

  action(sc_probeShrCoreData, "sc", desc="probe shared cores, return data") {
    peek(requestNetwork_in, CPURequestMsg) { // not the right network?
      NetDest probe_dests;
      bool noProbe := false;
      bool unicast := false;
      MachineID owner;
      if(ownerTracking || sharerTracking) {
        CacheEntry entry := static_cast(CacheEntry, 
            "pointer", ProbeFilterMemory.lookup(address));
        noProbe := entry.pfState == ProbeFilterState:I ||
                   entry.pfState == ProbeFilterState:S || 
                   entry.pfState == ProbeFilterState:O && tbe.OriginalRequestor == entry.Owner && (entry.Sharers.isEmpty());
        // only restrict probe to owner instead of broadcast
        unicast := entry.pfState == ProbeFilterState:O;
        owner := entry.Owner;
      }

      // Add relevant machine types based on CPUonly, GPUonly and noTCCdir:
      //  CPUonly &&  GPUonly -> Invalid
      //  CPUonly && !GPUonly -> Add CorePairs
      // !CPUonly &&  GPUonly -> Add TCCs or TCC dirs
      // !CPUonly && !GPUonly -> Add CorePairs and TCCs or TCC dirs
      if (CPUonly) {
        assert(!GPUonly);
        if (!ownerTracking) {
          probe_dests.broadcast(MachineType:CorePair);
        }
        else if((ownerTracking || sharerTracking) && !noProbe) {
          if(unicast) {
            probe_dests.add(owner);
          } else {
            assert(false); // should not enter here
          }
        } else {
          DPRINTF(OPT4, "Skipping probing CorePair because it does not have modified line\n");
        }
      } else {
        // CPU + GPU system
        if (!GPUonly) {
          if (!ownerTracking) {
            probe_dests.broadcast(MachineType:CorePair);
          }
          else if((ownerTracking || sharerTracking) && !noProbe) {
            if(unicast) {
              DPRINTF(OPT4, "ProbeUnicast\n");
              probe_dests.add(owner);
            } else {
              assert (false); // should not enter here
            }
          } else {
            DPRINTF(OPT4, "Skipping probing CorePair because it does not have line\n");
          }
        }

        // CPU + GPU or GPU only system
        // We don't need to notify TCC about reads
        // this is conservative, might be able to elide probes here as well
        if (!noTCCdir) {
          if(!ownerTracking || (ownerTracking || sharerTracking) && !noProbe) {
            probe_dests.add(mapAddressToRange(address, MachineType:TCCdir,
                                            TCC_select_low_bit,
                                            TCC_select_num_bits));
          } else {
            DPRINTF(OPT4, "Skipping probing GPU because we don't care to notify it\n");
          }
        }
      }
      probe_dests.remove(in_msg.Requestor);

      if (probe_dests.count() > 0) {
        enqueue(probeNetwork_out, NBProbeRequestMsg, response_latency) {
          out_msg.addr := address;
          out_msg.Type := ProbeRequestType:PrbDowngrade;
          out_msg.ReturnData := true;
          out_msg.MessageSize := MessageSizeType:Control;
          out_msg.Destination := probe_dests;
          tbe.NumPendingAcks := out_msg.Destination.count();
          out_msg.isGLCSet := in_msg.isGLCSet;
          out_msg.isSLCSet := in_msg.isSLCSet;
          DPRINTF(RubySlicc, "%s\n", (out_msg));
          APPEND_TRANSITION_COMMENT(" sc: Acks remaining: ");
          APPEND_TRANSITION_COMMENT(tbe.NumPendingAcks);
          tbe.ProbeRequestStartTime := curCycle();
          assert(out_msg.Destination.count() > 0);
        }
      }

      if (probe_dests.count() == 0) {
        enqueue(triggerQueue_out, TriggerMsg, 1) {
          out_msg.addr := address;
          out_msg.Type := TriggerType:AcksComplete;
        }
      }
    }
  }

  // Only used during CtoD which is never invoked!
  action(ic_probeInvCore, "ic", desc="probe invalidate core, no return data needed") {
    peek(requestNetwork_in, CPURequestMsg) { // not the right network?
      NetDest probe_dests;
      bool NT := false;
      if(ownerTracking) {
        CacheEntry entry := static_cast(CacheEntry, 
            "pointer", ProbeFilterMemory.lookup(address));
        NT := entry.pfState == ProbeFilterState:I;
      }

      // Add relevant machine types based on CPUonly, GPUonly and noTCCdir:
      //  CPUonly &&  GPUonly -> Invalid
      //  CPUonly && !GPUonly -> Add CorePairs
      // !CPUonly &&  GPUonly -> Add TCCs or TCC dirs
      // !CPUonly && !GPUonly -> Add CorePairs and TCCs or TCC dirs
      if (CPUonly) {
        assert(!GPUonly);
        if (!ownerTracking || ownerTracking && !NT) {
          probe_dests.broadcast(MachineType:CorePair);
        } else {
          DPRINTF(OPT4, "Skipping probing CorePair because it does not have line\n");
        }
      } else {
        // CPU + GPU system
        if (!GPUonly) {
          if (!ownerTracking || ownerTracking && !NT) {
            probe_dests.broadcast(MachineType:CorePair);
          } else {
            DPRINTF(OPT4, "Skipping probing CorePair because it does not have line\n");
          }
        }

        // CPU + GPU or GPU only system
        if (noTCCdir) {
          if (!ownerTracking || ownerTracking && !NT) {
          probe_dests.add(mapAddressToRange(address, MachineType:TCC,
                                            TCC_select_low_bit,
                                            TCC_select_num_bits));
          } else {
            DPRINTF(OPT4, "Skipping probing TCC because it does not have line\n");
          }
        } else {
          if (!ownerTracking || ownerTracking && !NT) {
          probe_dests.add(mapAddressToRange(address, MachineType:TCCdir,
                                            TCC_select_low_bit,
                                            TCC_select_num_bits));
          } else {
            DPRINTF(OPT4, "Skipping probing TCC because it does not have line\n");
          }
        }
      }
      probe_dests.remove(in_msg.Requestor);

      if (probe_dests.count() > 0) {
        enqueue(probeNetwork_out, NBProbeRequestMsg, response_latency) {
          out_msg.addr := address;
          out_msg.Type := ProbeRequestType:PrbInv;
          out_msg.ReturnData := false;
          out_msg.MessageSize := MessageSizeType:Control;
          out_msg.Destination := probe_dests;
          out_msg.isGLCSet := in_msg.isGLCSet;
          out_msg.isSLCSet := in_msg.isSLCSet;
          tbe.NumPendingAcks := out_msg.Destination.count();
          APPEND_TRANSITION_COMMENT(" ic: Acks remaining: ");
          APPEND_TRANSITION_COMMENT(tbe.NumPendingAcks);
          DPRINTF(RubySlicc, "%s\n", out_msg);
          tbe.ProbeRequestStartTime := curCycle();
          assert(out_msg.Destination.count() > 0);
        }
      }

      if (probe_dests.count() == 0) {
        enqueue(triggerQueue_out, TriggerMsg, 1) {
          out_msg.addr := address;
          out_msg.Type := TriggerType:AcksComplete;
        }
      }
    }
  }

  action(d_writeDataToMemory, "d", desc="Write data to memory") {
    peek(responseNetwork_in, ResponseMsg) {
      // here too, memory write can be saved; only writeback to L3
      // this action is entirely just a stat recorder
      if (noWBCleanVictimsToMem) {
        // subsequent L3 write will still read dirtyness from in_msg
        // If an intermediate GPU write has dirtied L3, would the
        //      victim be written back without incorporating GPU's stuff?
        //      The way it's done now, that's how it is
        // Ans: There cannot be intermediate GPU writes
        // have to update the TBE, too, because of how this
        // directory deals with functional writes
        tbe.Dirty   := in_msg.Dirty;
        tbe.DataBlk := in_msg.DataBlk;
        DPRINTF(OPT2, "MemWrite saved; only L3 will be updated\n");
      } else { // default
        enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
          out_msg.addr := address;
          out_msg.Type := MemoryRequestType:MEMORY_WB;
          out_msg.Sender := machineID;
          out_msg.MessageSize := MessageSizeType:Writeback_Data;
          out_msg.DataBlk := in_msg.DataBlk;
        }
        if (tbe.Dirty == false) {
            // have to update the TBE, too, because of how this
            // directory deals with functional writes
          tbe.DataBlk := in_msg.DataBlk;
        }
      }
    }
  }

  action(f_writeFlushDataToMemory, "f", desc="Write flush data to memory") {
    peek(requestNetwork_in, CPURequestMsg) {
      enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
        out_msg.addr := address;
        out_msg.Type := MemoryRequestType:MEMORY_WB;
        out_msg.Sender := machineID;
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
        out_msg.DataBlk := in_msg.DataBlk;
      }
      if (tbe.Dirty == false) {
          // have to update the TBE, too, because of how this
          // directory deals with functional writes
        tbe.DataBlk := in_msg.DataBlk;
      }
    }
  }

  action(atd_allocateTBEforDMA, "atd", desc="allocate TBE Entry for DMA") {
    check_allocate(TBEs);
    peek(dmaRequestQueue_in, DMARequestMsg) {
      TBEs.allocate(address);
      set_tbe(TBEs.lookup(address));
      tbe.OriginalRequestor := in_msg.Requestor;
      tbe.NumPendingAcks := 0;
      tbe.Dirty := false;
      tbe.Len := in_msg.Len;
      if (in_msg.Type == DMARequestType:WRITE) {
        tbe.TBEState := State:BDW_PM;
        tbe.wtData := true;
        tbe.Dirty := true;
        tbe.DataBlk := in_msg.DataBlk;
        // DMAs can be partial cache line. This is indicated by a "Len" value
        // which is non-zero. In this case make sure to set the writeMask
        // appropriately. This can occur for either reads or writes.
        if (in_msg.Len != 0) {
          tbe.writeMask.setMask(addressOffset(in_msg.PhysicalAddress, address), in_msg.Len);
        } else {
          tbe.writeMask.fillMask();
        }
      }
    }
  }

  action(t_allocateTBE, "t", desc="allocate TBE Entry") {
    check_allocate(TBEs);
    peek(requestNetwork_in, CPURequestMsg) {
      TBEs.allocate(address);
      set_tbe(TBEs.lookup(address));
      if (in_msg.Type == CoherenceRequestType:WriteThrough) {
        tbe.writeMask.clear();
        tbe.writeMask.orMask(in_msg.writeMask);
        tbe.wtData := true;
        tbe.CURequestor := in_msg.CURequestor;
        tbe.LastSender := in_msg.Requestor;
      }
      if (in_msg.Type == CoherenceRequestType:Atomic ||
          in_msg.Type == CoherenceRequestType:AtomicReturn ||
          in_msg.Type == CoherenceRequestType:AtomicNoReturn) {
        tbe.writeMask.clear();
        tbe.writeMask.orMask(in_msg.writeMask);
        tbe.atomicData := true;
        if (in_msg.Type == CoherenceRequestType:AtomicReturn) {
          tbe.atomicDataReturn := true;
        } else {
          assert(in_msg.Type == CoherenceRequestType:AtomicNoReturn);
          tbe.atomicDataNoReturn := true;
        }
        tbe.CURequestor := in_msg.CURequestor;
        tbe.LastSender := in_msg.Requestor;
        tbe.isSLCSet := in_msg.isSLCSet;
      }
      // GPU read requests also need to track where the requestor came from
      if (in_msg.Type == CoherenceRequestType:RdBlk) {
        tbe.CURequestor := in_msg.CURequestor;
      }
      tbe.Dirty := false;
      // in case of vicClean/vicDirty, there is no info on Dirty
      // it will be updated when the victimized data arrives on a different n/w
      if (in_msg.Type == CoherenceRequestType:WriteThrough) {
        tbe.DataBlk.copyPartial(in_msg.DataBlk,in_msg.writeMask);
        tbe.Dirty := true;
      }
      tbe.OriginalRequestor := in_msg.Requestor; // includes both GPU and CPU caches
      tbe.NumPendingAcks := 0;
      tbe.Cached := in_msg.ForceShared;
      tbe.InitialRequestTime := in_msg.InitialRequestTime;
      tbe.isGLCSet := in_msg.isGLCSet;
      tbe.isSLCSet := in_msg.isSLCSet;
      DPRINTF(RubySlicc, "t_allocateTBE in_msg: %s, tbe: %s\n", in_msg, tbe.CURequestor);
      tbe.TBEState := State:B_PM;
      if (in_msg.Type == CoherenceRequestType:WriteThrough ||
          in_msg.Type == CoherenceRequestType:Atomic ||
          in_msg.Type == CoherenceRequestType:AtomicReturn ||
          in_msg.Type == CoherenceRequestType:AtomicNoReturn ||
          in_msg.Type == CoherenceRequestType:RdBlkM) {
        tbe.TBEState := State:BM_PM;
      } else if (in_msg.Type == CoherenceRequestType:RdBlkS) {
        tbe.TBEState := State:BS_PM;
      } else if (in_msg.Type == CoherenceRequestType:RdBlk) {
        tbe.TBEState := State:B_PM;
      } else if (in_msg.Type == CoherenceRequestType:VicDirty ||
                 in_msg.Type == CoherenceRequestType:VicClean) {
        tbe.TBEState := State:BL;
      } else if (in_msg.Type == CoherenceRequestType:WriteFlush) {
        tbe.TBEState := State:F;
      } else if (ownerTracking || sharerTracking) { // TODO condition for PF_Repl
        tbe.TBEState := State:B_P;
        DPRINTF(OPT4, "TBEState set to B_P\n");
      } else {
        DPRINTF(RubySlicc, "Error: Incomplete switch\n");
      }
      tbe.responded := false;
      tbe.readyToResp := false;
    }
  }

  action(dt_deallocateTBE, "dt", desc="deallocate TBE Entry") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  action(wd_writeBackData, "wd", desc="Write back data if needed") {
    /*  Write to memory only if wtData, atomicData and not writing to L3
     *  or tbe is dirty because of memdata or l3hit or probeComplete
     *    which can be due to dirty probe responses received
     *    or L3 hit and it's dirty -- check this, and also seems like you're not writing data to LLC in most cases and also end up not writing to memory, so you might be missing updates made to lines -- see if you only not write to mem or llc if clean -- that might be alright though
     */
    if (
          !noWBCleanVictimsToMem && (tbe.wtData || tbe.atomicData || (tbe.Dirty == false)) ||
           noWBCleanVictimsToMem && (tbe.Dirty  || tbe.Dirty)
       ) {
      // Only perform atomics in the directory if the SLC bit is set, or
      // if the L2 is WT
      if(earlyDataToCore && !tbe.responded || !earlyDataToCore) { 
        /* it is important to only do this when the requesting-core hasn't
         * already been responded to so as not to repeat the operation twice
         * which in some ATOMIC_RETURN cases could duplicate a log entry
         * leading to problems when processing the response packets at the GPUColaescer
         */
        if (tbe.atomicData && (tbe.isSLCSet || !L2isWB)) {
          if (tbe.atomicDataReturn) {
            tbe.DataBlk.atomicPartial(tbe.DataBlk, tbe.writeMask, false);
          } else {
            assert(tbe.atomicDataNoReturn);
            tbe.DataBlk.atomicPartial(tbe.DataBlk, tbe.writeMask, true);
          }
        }
      }
      // Skipping writing to memory blindly, a compulsorily following 
      // action (alwt) will determine if it's necessary to write back data.
      // Exception: DMA writes -- they do not write to L3, so memory writeback
      // might be necessary for those;
      // the above action on atomicData will still be needed though
      if (
            noWBCleanVictimsToMem && 
                ((tbe.TBEState == State:BDW_PM) || tbe.Dirty) 
            || !noWBCleanVictimsToMem
         ) {
        DPRINTF(RubySlicc, "Optimization not turned on, or overriding mem write; tbe.TBEState = %s\n", tbe.TBEState);
        enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
          out_msg.addr := address;
          out_msg.Type := MemoryRequestType:MEMORY_WB;
          out_msg.Sender := machineID;
          out_msg.MessageSize := MessageSizeType:Writeback_Data;
          out_msg.DataBlk := tbe.DataBlk;
          DPRINTF(ProtocolTrace, "%s\n", out_msg);
        }
      } else {
        DPRINTF(OPT2, "MemWrite saved\n");
      }
    }
  }

  action(mt_writeMemDataToTBE, "mt", desc="write Mem data to TBE") {
    peek(memQueue_in, MemoryMsg) {
      if (tbe.wtData == true) {
        // Keep the write-through data based on mask, but use the memory block
        // for the masked-off data. If we received a probe with data, the mask
        // will be filled and the tbe data will fully overwrite the memory
        // data in the temp block.
        DataBlock tmpBlk := in_msg.DataBlk;
        tmpBlk.copyPartial(tbe.DataBlk, tbe.writeMask);
        tbe.DataBlk := tmpBlk;
      } else if (tbe.Dirty == false) {
        tbe.DataBlk := in_msg.DataBlk; //in_msg here is from memQueue_in
      }
      tbe.MemData := true;
    }
  }

  action(y_writeProbeDataToTBE, "y", desc="write Probe Data to TBE") {
    peek(responseNetwork_in, ResponseMsg) {
      if (in_msg.Dirty) {
        if(earlyDataToCore) {
          tbe.readyToResp := true;
        }
        if (tbe.wtData) {
          DataBlock tmp := in_msg.DataBlk;
          tmp.copyPartial(tbe.DataBlk,tbe.writeMask);
          tbe.DataBlk := tmp;
          tbe.writeMask.fillMask();
        } else if (tbe.Dirty) {
          // assert(false); 
          // Double data possible with multi owned dirty L2s
          // double data is impossible
          if(tbe.atomicData == false && tbe.wtData == false) {
            assert(tbe.atomicDataReturn == false && tbe.atomicDataNoReturn == false);
            DPRINTF(RubySlicc, "Got double data for 0x%x from %s\n", address, in_msg.Sender);
            assert(tbe.DataBlk == in_msg.DataBlk);  // in case of double data
          }
        } else {
          tbe.DataBlk := in_msg.DataBlk;
          tbe.Dirty := true;
          tbe.LastSender := in_msg.Sender;
        }
      }
      if (in_msg.Hit) {
        tbe.Cached := true; 
        // indicates line was cached in L2 -- 
        //   for exclusive perm determination for the receiving L2 cache
      }
    }
  }

  action(edc_earlyDataToCore, "edc", desc="early data to core") {
    if(earlyDataToCore) {
      if(tbe.readyToResp && !tbe.responded && !(tbe.TBEState == State:B_P)) {
        DPRINTF(OPT1, "Early: ReadyToResp; tbe.TBEState %d\n", tbe.TBEState);
        //if (tbe.TBEState == State:BM_PM || tbe.TBEState == State:BM_Pm) {
        //  DPRINTF(OPT1, "Early: Responding to RdBlkM/WT/Atomic at %d probes\n", tbe.NumPendingAcks);
        //  /* this would otherwise have been done before
        //   * responding to the core, as part of writing back to memory
        //   * since we are skipping that part now, we need to do that here
        //   */
        //  if (tbe.atomicData && (tbe.isSLCSet || !L2isWB)) {
        //    if (tbe.atomicDataReturn) {
        //      tbe.DataBlk.atomicPartial(tbe.DataBlk, tbe.writeMask, false);
        //    } else {
        //      assert(tbe.atomicDataNoReturn);
        //      tbe.DataBlk.atomicPartial(tbe.DataBlk, tbe.writeMask, true);
        //    }
        //  }

        //  if (tbe.wtData) {
        //    enqueue(triggerQueue_out, TriggerMsg, 1) {
        //      out_msg.addr := address;
        //      out_msg.Type := TriggerType:UnblockWriteThrough;
        //    }
        //  } else {
        //    enqueue(responseNetwork_out, ResponseMsg, response_latency) {
        //      out_msg.addr := address;
        //      out_msg.Type := CoherenceResponseType:NBSysResp;
        //      if (tbe.L3Hit) {
        //        out_msg.Sender := createMachineID(MachineType:L3Cache, intToID(0));
        //      } else {
        //        out_msg.Sender := machineID; // TODO memory (L3 miss)
        //      }
        //      out_msg.Destination.add(tbe.OriginalRequestor);
        //      out_msg.DataBlk := tbe.DataBlk;
        //      out_msg.MessageSize := MessageSizeType:Response_Data;
        //      out_msg.Dirty := tbe.Dirty;
        //      out_msg.State := CoherenceState:Modified;
        //      out_msg.CtoD := false;
        //      out_msg.InitialRequestTime := tbe.InitialRequestTime;
        //      out_msg.ForwardRequestTime := tbe.ForwardRequestTime;
        //      out_msg.ProbeRequestStartTime := tbe.ProbeRequestStartTime;
        //      out_msg.OriginalResponder := tbe.LastSender;
        //      out_msg.isGLCSet := tbe.isGLCSet;
        //      out_msg.isSLCSet := tbe.isSLCSet;
        //      if(tbe.atomicData){
        //        out_msg.CURequestor := tbe.CURequestor;
        //      }
        //      out_msg.L3Hit := tbe.L3Hit;
        //      DPRINTF(RubySlicc, "%s\n", out_msg);
        //    }
        //    if (tbe.atomicData) {
        //      enqueue(triggerQueue_out, TriggerMsg, 1) {
        //        out_msg.addr := address;
        //        out_msg.Type := TriggerType:UnblockWriteThrough;
        //      }
        //    }
        //  }
        //} else if (tbe.TBEState == State:BS_PM || tbe.TBEState == State:BS_Pm) {
        if (tbe.TBEState == State:BS_PM || tbe.TBEState == State:BS_Pm) {
          DPRINTF(OPT1, "Early: Responding to RdBlkS at %d probes\n", tbe.NumPendingAcks);
          enqueue(responseNetwork_out, ResponseMsg, response_latency) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:NBSysResp;
            if (tbe.L3Hit) {
              out_msg.Sender := createMachineID(MachineType:L3Cache, intToID(0));
            } else {
              out_msg.Sender := machineID;
            }
            out_msg.Destination.add(tbe.OriginalRequestor);
            out_msg.DataBlk := tbe.DataBlk;
            out_msg.MessageSize := MessageSizeType:Response_Data;
            out_msg.Dirty := false;
            out_msg.State := CoherenceState:Shared;
            out_msg.InitialRequestTime := tbe.InitialRequestTime;
            out_msg.ForwardRequestTime := tbe.ForwardRequestTime;
            out_msg.ProbeRequestStartTime := tbe.ProbeRequestStartTime;
            out_msg.OriginalResponder := tbe.LastSender;
            out_msg.L3Hit := tbe.L3Hit;
            out_msg.CURequestor := tbe.CURequestor;
            DPRINTF(RubySlicc, "%s\n", out_msg);
          }
        } else if (tbe.TBEState == State:B_PM || tbe.TBEState == State:B_Pm) {
          DPRINTF(OPT1, "Early: Responding to RdBlk at %d probes\n", tbe.NumPendingAcks);
          enqueue(responseNetwork_out, ResponseMsg, response_latency) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:NBSysResp;
            if (tbe.L3Hit) {
              out_msg.Sender := createMachineID(MachineType:L3Cache, intToID(0));
            } else {
              out_msg.Sender := machineID;
            }
            out_msg.Destination.add(tbe.OriginalRequestor);
            out_msg.DataBlk := tbe.DataBlk;
            out_msg.MessageSize := MessageSizeType:Response_Data;
            out_msg.Dirty := tbe.Dirty;
            // this is ignored by GPU
            if (tbe.Cached) {
              out_msg.State := CoherenceState:Shared;
            } else {
              out_msg.State := CoherenceState:Exclusive;
            }
            out_msg.InitialRequestTime := tbe.InitialRequestTime;
            out_msg.ForwardRequestTime := tbe.ForwardRequestTime;
            out_msg.ProbeRequestStartTime := tbe.ProbeRequestStartTime;
            out_msg.OriginalResponder := tbe.LastSender;
            out_msg.L3Hit := tbe.L3Hit;
            out_msg.isGLCSet := tbe.isGLCSet;
            out_msg.isSLCSet := tbe.isSLCSet;
            out_msg.CURequestor := tbe.CURequestor;
            DPRINTF(RubySlicc, "%s\n", out_msg);
          }
          tbe.responded := true;
        }
        tbe.responded := true;
      } else {
        //DPRINTF(OPT1, "Early prbResp NA for PF_Repl\n"); // usually for B_P (PF_Repl)
      }
    }
  }

  action(mwc_markSinkWriteCancel, "mwc", desc="Mark to sink impending VicDirty") {
    peek(responseNetwork_in, ResponseMsg) {
      getDirectoryEntry(address).VicDirtyIgnore.add(in_msg.Sender);
      APPEND_TRANSITION_COMMENT(" setting bit to sink VicDirty ");
    }
  }

  action(x_decrementAcks, "x", desc="decrement Acks pending") {
    tbe.NumPendingAcks := tbe.NumPendingAcks - 1;
    APPEND_TRANSITION_COMMENT(" Acks remaining: ");
    APPEND_TRANSITION_COMMENT(tbe.NumPendingAcks);
  }

  action(o_checkForCompletion, "o", desc="check for ack completion") {
    if (tbe.NumPendingAcks == 0) {
      enqueue(triggerQueue_out, TriggerMsg, 1) {
        out_msg.addr := address;
        out_msg.Type := TriggerType:AcksComplete;
      }
      APPEND_TRANSITION_COMMENT(" Triggered ProbeAcksComplete \n");
    }
  }

  action(rv_removeVicDirtyIgnore, "rv", desc="Remove ignored core") {
    peek(requestNetwork_in, CPURequestMsg) {
      getDirectoryEntry(address).VicDirtyIgnore.remove(in_msg.Requestor);
    }
  }

  action(al_allocateL3Block, "al", desc="allocate the L3 block on WB") {
    /* optimization 3 (risky!): not writing back clean victims to L3
     * naively, it's easy to see how it would perform worse because the clean victims are
     * lost "in the air" as L3 -- being a victim cache -- would not have cached it on the
     * refill path, and the next time a read/write miss is incurred for those lines, 
     * directorywill now need to fetch from memory. My argument is that if there's ever 
     * data that needs to be evicted from CPU to GPU, what matters is the data that CPU 
     * modifies (victims will hence be dirty). I don't see any reason why CPU would read 
     * a bunch and not modify it and evict it and the GPU ends upneeding it, unless it's an
     * atomic variable, which is handled differently anyway. 
     * Even data that's evicted as an unintended consequence 
     * (like capacity-induced writeback) is also not readily evident if needed by the GPU
     * best case, 
          this saves an L3 write (significant energy in silicon) 
          and potentially a memory write, 
          less pollution, 
          increased effective cache capacity
     * worst case, 
          if GPU ends up needing it, it would then have the dir fetch from mem
     */
    peek(responseNetwork_in, ResponseMsg) {
      if (!noWBCleanVictimsToLLC || (noWBCleanVictimsToLLC && in_msg.Dirty)) {
        if (L3CacheMemory.isTagPresent(address)) {
          CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(address));
          APPEND_TRANSITION_COMMENT(" al wrote data to L3 (hit) ");
          entry.DataBlk := in_msg.DataBlk;
          entry.LastSender := in_msg.Sender;
          // in case the entry is already dirty to begin with 
          // (due to a prior dirty write, not yet written back)
          // retain the dirty bit, even if this victim is clean
          // doesn't matter anyway since dataBlk is overwriting 
          // on a separate note, if victim being processed is dirty
          // incorpoate that too 
          // so the solution is to OR entry.Dirty with in_msg.Dirty
          // TODO does the consistency model not care?
          entry.Dirty := entry.Dirty || in_msg.Dirty;
          assert(is_valid(tbe));
          //The controller always allocates a TBE entry upon receipt of a request from L2 caches.
          //L3Hit flag is used by the hit profiling action pr_profileL3HitMiss to determine hit or miss.
          //A TBE entry is not deallocated until a request is fully serviced and profiled.
          tbe.L3Hit := true;
          // tbe is never used for this transaction
        } else {
          if (L3CacheMemory.cacheAvail(address) == false) {
            Addr victim := L3CacheMemory.cacheProbe(address);
            CacheEntry victim_entry := static_cast(CacheEntry, "pointer",
                                                   L3CacheMemory.lookup(victim));
            if (wbLLC && victim_entry.Dirty || !wbLLC) {
              enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
                out_msg.addr := victim;
                out_msg.Type := MemoryRequestType:MEMORY_WB;
                out_msg.Sender := machineID;
                out_msg.MessageSize := MessageSizeType:Writeback_Data;
                out_msg.DataBlk := victim_entry.DataBlk;
              }
            } else {
              // do not write-back, directly deallocate
              // since it's guaranteed memory has not diverged from L3
              DPRINTF(OPT2, "MemWrite saved because LLC victim not dirty\n");
            }
            L3CacheMemory.deallocate(victim);
          }
          assert(L3CacheMemory.cacheAvail(address));
          CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.allocate(address, new CacheEntry));
          APPEND_TRANSITION_COMMENT(" al wrote data to L3 ");
          entry.DataBlk := in_msg.DataBlk;

          entry.LastSender := in_msg.Sender;
          entry.Dirty := in_msg.Dirty; // fresh entry, explicitify dirtiness
        }
      } else { // never entered on !noWBCleanVictimsToLLC
               // or if L2 is sending clean victim -- this case leads to lost lines, risky one
        DPRINTF(OPT3, "L2CleanVictimToL3 not written\n");
        // TODO how to capture the consequential read misses?
      }
    }
  }

  action(alwt_allocateL3BlockOnWT, "alwt", desc="allocate the L3 block on WT") {
    /* Since LLC is a victim cache, reads misses are not cached there, only victim evictions from L2s;
     * This also means LLC is not an inclusive cache -- it excludes read missed blocks.
     * TODO carefully evaluate whether it's better to probe-broadcast/probe-unicast/fetch-from-LLC 
     * when encountering a rd miss at dir -- previous rd miss to same addr may not have cached it in LLC
     * so if you fetch from LLC, it'll initiate a mem read
     * probes could be easier to fetch from if you can probe-unicast
     */
    if ((tbe.wtData || tbe.atomicData) && useL3OnWT || tbe.TBEState == State:B_P) {
      if (tbe.TBEState == State:B_P) {
        DPRINTF(OPT4, "Writing to LLC in case of PF_Repl backProbe\n");
        // Memory hasn't been written to for B_P -- this might be violating "victim-cachedness" of the LLC
      }
      //This tag check does not need to be counted as a hit or Miss, it has already been recorded.
      if (L3CacheMemory.isTagPresent(address)) {
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.lookup(address));
        APPEND_TRANSITION_COMMENT(" al wrote data to L3 (hit) ");
        entry.DataBlk := tbe.DataBlk;
        entry.LastSender := tbe.LastSender;
        // this ensures dirty probes are recognized
        // and also carries forward the original dirtyness of the line
        entry.Dirty := entry.Dirty || tbe.Dirty; 
        // atomic can be just a read, so not necessarily dirty
      } else {
        if (L3CacheMemory.cacheAvail(address) == false) {
          Addr victim := L3CacheMemory.cacheProbe(address);
          CacheEntry victim_entry := static_cast(CacheEntry, "pointer",
                                                 L3CacheMemory.lookup(victim));
          if (noWBCleanVictimsToMem && victim_entry.Dirty || !noWBCleanVictimsToMem) {
            /* This condition does not help improve performance since
             * we are not waiting for the WBAck from memory anyway
             * However, the memory traffic will be greatly reduced.
             * However, since we are skipping updating memory when clean,
             * we may need to wait for the acknowledgement lest memory and L3 diverge,
             * in which case, still, L3 would have the latest copy.
             */
            enqueue(memQueue_out, MemoryMsg, to_memory_controller_latency) {
              out_msg.addr := victim;
              out_msg.Type := MemoryRequestType:MEMORY_WB;
              out_msg.Sender := machineID;
              out_msg.MessageSize := MessageSizeType:Writeback_Data;
              out_msg.DataBlk := victim_entry.DataBlk;
            }
            // TODO do we need to wait for WBAck?
          } else {
            // else, drop WB because memory and clean L3 are 
            // by definition reconciled
            DPRINTF(OPT2, "MemWrite saved\n");
          }
          L3CacheMemory.deallocate(victim);
        }
        assert(L3CacheMemory.cacheAvail(address));
        CacheEntry entry := static_cast(CacheEntry, "pointer", L3CacheMemory.allocate(address, new CacheEntry));
        APPEND_TRANSITION_COMMENT(" al wrote data to L3 ");
        entry.DataBlk := tbe.DataBlk;
        entry.LastSender := tbe.LastSender;
        entry.Dirty := tbe.Dirty; // same reason as in the L3 victimization case above
        /* because the new allocation can now be dirty, the memory writeback can be skipped
         * Next time an L3 line is victimized, it will only be written back iff also dirty
         */
      }
    }
  }

  action(apf_allocateProbeFilterEntry, "apf", desc="Allocate probe filte entry") {
    // During PF_Repl event, the line being operated on corresponds to PF victim
    // If servicing RdBlk*, ... to requested line
    if(ownerTracking || sharerTracking) {
      if (!ProbeFilterMemory.isTagPresent(address)) {
        // PF_Repl cannot enter since tag (=PF victim) has to be present
        // Something can enter only when no tagMatch, 
        //   but cacheAvail has to be true (yes, think carefully)
        //   otherwise PF_Repl should have been called
        assert(ProbeFilterMemory.cacheAvail(address));
        DPRINTF(OPT4, "RdBlk* or Atomic/WT received and CacheAvail, so preallotting PF entry\n");
        CacheEntry entry := static_cast(CacheEntry, 
            "pointer", ProbeFilterMemory.allocate(address, new CacheEntry));
        APPEND_TRANSITION_COMMENT(" allocating a new probe filter entry since tag no match & cacheAvail ");
        // will be updated on the return path of a line
        entry.pfState := ProbeFilterState:I;
        assert(entry.Sharers.isEmpty());
        DPRINTF(OPT5, "SHL: [0x%x (victim/not)] Initialized sharersList\n", address);
      } else {
        /* TagMatch -- either for the victim during PF_Repl
         *   or for RdBlkX/WT/Atomic during original request
         * If there's a tagMatch, you may need to probe (one exception), cause line in system
         */
        DPRINTF(OPT4, "PF tag present either for new req or PF_Repl; may need to probe\n");
      }
    }
  }

  action(mpfe_markPFEntryForEviction, "mpfe", desc="Mark this PF entry is being evicted") {
    assert(ProbeFilterMemory.isTagPresent(address));
    CacheEntry entry := static_cast(CacheEntry, "pointer", ProbeFilterMemory.lookup(address));
    entry.pfState := ProbeFilterState:B;
    peek(requestNetwork_in, CPURequestMsg) {
      tbe.demandAddress := in_msg.addr;
    }
  }

  action(we_wakeUpEvictionDependents, "we", desc="Wake up requests waiting for demand address and victim address") {
    wakeUpBuffers(address);
    wakeUpBuffers(tbe.demandAddress);
  }

  action(dpf_deallocateProbeFilter, "dpf", desc="deallocate PF entry") {
    assert(ProbeFilterMemory.isTagPresent(address));
    ProbeFilterMemory.deallocate(address);
  }

  action(cupf_conditionalUpdatePF, "cupf", desc="") {
    if (sharerTracking) {
      if(tbe.cupfOn) {
        assert(ProbeFilterMemory.isTagPresent(address));
        CacheEntry entry := static_cast(CacheEntry, "pointer", ProbeFilterMemory.lookup(address));
        ProbeFilterState origState := entry.pfState;
        DPRINTF(OPT5, "SHL [0x%x] Conditionally updating PF: origState:%s owner:%s tbe.Dirty? %d sharerList:%s\n", address, origState, entry.Owner, tbe.Dirty, entry.Sharers);
        if (origState == ProbeFilterState:O) {
              if(!tbe.Dirty) {
                // tbe.Dirty can only be raised if probe response was dirty
                // tbe.Dirty is otherwise only set for WT
                // remember when a line is O it could mean E (and clean) cause of our conservative expectation.
                // E when probe-downgraded, will change to S, not O. So check tbe.Dirty
                entry.pfState := ProbeFilterState:S;
                entry.Sharers.add(tbe.OriginalRequestor);
                entry.Sharers.add(entry.Owner);
                DPRINTF(OPT5, "SHL [0x%x] Conditionally updated PF: newState:%s owner:%s tbe.Dirty? %d sharerList:%s\n", address, origState, entry.Owner, tbe.Dirty, entry.Sharers);
              }
        }
      }
    }
  }

        
  action(upf_updateProbeFilter, "upf", desc="") {
    if (ownerTracking || sharerTracking) {
      peek(requestNetwork_in, CPURequestMsg) {
        assert(ProbeFilterMemory.isTagPresent(address));
        CacheEntry entry := static_cast(CacheEntry, "pointer", ProbeFilterMemory.lookup(address));
        ProbeFilterState origState := entry.pfState;
        DPRINTF(OPT5, "SHL [0x%x] Updating PF: origState:%s owner:%s sharerList:%s\n", address, origState, entry.Owner, entry.Sharers);
        if (origState == ProbeFilterState:I) {
          if(    in_msg.Type == CoherenceRequestType:WriteThrough
              || in_msg.Type == CoherenceRequestType:Atomic
              || in_msg.Type == CoherenceRequestType:AtomicReturn
              || in_msg.Type == CoherenceRequestType:AtomicNoReturn
              || in_msg.Type == CoherenceRequestType:RdBlkM
              || in_msg.Type == CoherenceRequestType:CtoD
              ) {
            entry.pfState := ProbeFilterState:O;
            entry.Owner := tbe.OriginalRequestor;
            // do not add to sharer list
            assert(!entry.Sharers.isElement(tbe.OriginalRequestor));
          } else if(in_msg.Type == CoherenceRequestType:RdBlkS) {
            entry.pfState := ProbeFilterState:S;
            if(sharerTracking) {
              entry.Sharers.add(tbe.OriginalRequestor);
            }
          } else if(in_msg.Type == CoherenceRequestType:RdBlk) {
            // dir needs to consider RdBlk as O if pfState was I and also update owner
            entry.pfState := ProbeFilterState:O;
            entry.Owner := tbe.OriginalRequestor;
            // do not add to sharer list
            assert(!entry.Sharers.isElement(entry.Owner));
          }
        } else if(origState == ProbeFilterState:S) {
          if(    in_msg.Type == CoherenceRequestType:WriteThrough
              || in_msg.Type == CoherenceRequestType:Atomic
              || in_msg.Type == CoherenceRequestType:AtomicReturn
              || in_msg.Type == CoherenceRequestType:AtomicNoReturn
              || in_msg.Type == CoherenceRequestType:RdBlkM
              || in_msg.Type == CoherenceRequestType:CtoD
              ) {
            entry.pfState := ProbeFilterState:O;
            entry.Owner := tbe.OriginalRequestor;
            // previous sharers are invalidated
            entry.Sharers.clear();
          } else if (in_msg.Type == CoherenceRequestType:RdBlkS) {
            if(sharerTracking) {
              // double add is ignored
              entry.Sharers.add(tbe.OriginalRequestor);
            }
            // need to artificially intimate requestor that line came from another cache
            // so it doesn't assume exclusivity when eliding probe when DirState S
            tbe.Cached := true;
          } else if(in_msg.Type == CoherenceRequestType:RdBlk) {
            // in the probe action, it will already be updated that the line was received from a cache
            //    despite it having been received from the LLC/Mem
            if(sharerTracking) {
              // Update: cannot happen. This case is handled internally to CorePair
              //if(entry.Sharers.isElement(tbe.OriginalRequestor) && entry.Sharers.count() == 1) {
              //  // previous-sharer is now requesting exclusivity, can be granted
              //  entry.pfState := ProbeFilterState:O;
              //  entry.Sharers.remove(tbe.OriginalRequestor);
              //  // need to artificially intimate requestor that line came from another cache
              //  // so it doesn't assume exclusivity when eliding probe when pfState S
              //  // TODO think if this needs to apply to owner tracking?
              //  tbe.Cached := true;
              //} else {
                // new requstor requesting share
                entry.Sharers.add(tbe.OriginalRequestor);
              //}
            }
          }
        } else if(origState == ProbeFilterState:O) {
          if(    in_msg.Type == CoherenceRequestType:WriteThrough
              || in_msg.Type == CoherenceRequestType:Atomic
              || in_msg.Type == CoherenceRequestType:AtomicReturn
              || in_msg.Type == CoherenceRequestType:AtomicNoReturn
              || in_msg.Type == CoherenceRequestType:RdBlkM
              || in_msg.Type == CoherenceRequestType:CtoD
              ) {
            entry.Owner := tbe.OriginalRequestor;
            // do not add to sharer list
            if(sharerTracking) {
              entry.Sharers.clear();
            }
          } else if( in_msg.Type == CoherenceRequestType:RdBlkS) {
            if(sharerTracking) {
              if(entry.Owner == tbe.OriginalRequestor) {
                // previously exclusive line is now shared
                // (because of an I$ miss)
                // If there was an intermediate RdBlk/RdBlkS from another requestor,
                // this line would have tuned S
                // So, it had to have remained exclusive
                if(!entry.Sharers.isEmpty()) {
                  DPRINTF(OPT5, "Sharer's list supposed to be empty, but not: %s\n", entry.Sharers);
                  assert(false);
                }
                entry.pfState := ProbeFilterState:S;
              }
              tbe.cupfOn := true;
              // add to the sharer list either way
              entry.Sharers.add(tbe.OriginalRequestor);
            }
          } else if (in_msg.Type == CoherenceRequestType:RdBlk) {
            // a line in E or O state cannot send an RdBlk request
            // it can only send a RdBlkS if in E
            if(machineIDToMachineType(in_msg.Requestor) == MachineType:CorePair) {
              // GPUs can silently relinquish ownership 
              assert(entry.Owner != tbe.OriginalRequestor);
            }
            if(sharerTracking) {
              if(entry.Owner != tbe.OriginalRequestor) {
                entry.Sharers.add(tbe.OriginalRequestor);
                tbe.cupfOn := true;
              } else {
                DPRINTF(OPT5, "Owner requested an RdBlk\n");
                assert (false);
              }
            }
          }
        } else {
          // TODO any missing cases?
          assert (false);
        }
        DPRINTF(OPT5, "SHL [0x%x] Final pfState:%s owner:%s sharersList: %s\n", address, entry.pfState, entry.Owner, entry.Sharers);
      }
    }
  }

  action(rmcd_removeSharerConditional, "rmcd", desc="remove sharer from probe Filter, conditional") {
    if(ownerTracking || sharerTracking) {
      assert(ProbeFilterMemory.isTagPresent(address));
      if(sharerTracking) {
        // ensure the victimized line is infact supposed to have been tracked in the dir
        CacheEntry entry := static_cast(CacheEntry, "pointer", ProbeFilterMemory.lookup(address));
        assert(entry.Sharers.isElement(tbe.OriginalRequestor) || entry.Owner == tbe.OriginalRequestor);
      }
    }
    if(sharerTracking) {
      peek(requestNetwork_in, CPURequestMsg) {
        if(machineIDToMachineType(in_msg.Requestor) == MachineType:CorePair) {
        }
        CacheEntry entry := static_cast(CacheEntry,
            "pointer", ProbeFilterMemory.lookup(address));
        entry.Sharers.remove(in_msg.Requestor);
        DPRINTF(OPT5, "SHL [0x%x] L2 sharer removed; sharersList: %s\n", address, entry.Sharers);
        if(entry.pfState == ProbeFilterState:O && in_msg.Type == CoherenceRequestType:VicDirty && !entry.Sharers.isEmpty()) {
          // owner is relinquishing dirty data
          entry.pfState := ProbeFilterState:S;
          DPRINTF(OPT5, "SHL Removed sole owner; pfState now S\n");
        }
        if(entry.Sharers.isEmpty()) {
          ProbeFilterMemory.deallocate(address);
          DPRINTF(OPT5, "SHL Removed sole sharer; invalidated line\n");
        }
      }
    }
  }

  action(sm_setMRU, "sm", desc="set probe filter entry as MRU") {
    if(ownerTracking || sharerTracking) {
      ProbeFilterMemory.setMRU(address);
    }
  }

  // these are only relevent when PF_Repl is recognized as an event
  // -- which is only true when ownerTracking is enabled
  action(bp_backProbe, "bp", desc="back probe") {
    DPRINTF(OPT4, "BackProbe\n");
    //  machineIDToMachineType() == MachineType:CorePair;
    NetDest probe_dests;
    if(ownerTracking || sharerTracking) {
      CacheEntry entry := static_cast(CacheEntry,
          "pointer", ProbeFilterMemory.lookup(address));
      if(sharerTracking) {
        probe_dests := entry.Sharers;
        DPRINTF(OPT5, "SHL [0x%x] BackProbing following sharers: %s\n", address, entry.Sharers);
      }
    }
    if (!sharerTracking && ownerTracking) {
      probe_dests.broadcast(MachineType:CorePair);
      if (!noTCCdir) {
        probe_dests.add(mapAddressToRange(address, MachineType:TCCdir,
                                      TCC_select_low_bit,
                                      TCC_select_num_bits));
      } else {
        probe_dests.add(mapAddressToRange(address, MachineType:TCC,
                                      TCC_select_low_bit,
                                      TCC_select_num_bits));
      }
    }

    tbe.NumPendingAcks := probe_dests.count();
    if (probe_dests.count() > 0) {
      enqueue(probeNetwork_out, NBProbeRequestMsg, response_latency) {
        out_msg.addr := address;
        out_msg.Type := ProbeRequestType:PrbInv;
        out_msg.ReturnData := true;
        out_msg.MessageSize := MessageSizeType:Control;

        out_msg.Destination := probe_dests;
        DPRINTF(RubySlicc, "%s\n", (out_msg));
        APPEND_TRANSITION_COMMENT(" bp: Expected Acks ");
        APPEND_TRANSITION_COMMENT(tbe.NumPendingAcks);
        tbe.ProbeRequestStartTime := curCycle();
      }
    }

    // backProbe was deemed necessary, so
    assert(probe_dests.count() > 0);
    if (probe_dests.count() == 0) {
      enqueue(triggerQueue_out, TriggerMsg, 1) {
        out_msg.addr := address;
        out_msg.Type := TriggerType:AcksComplete;
      }
    }
  }

  action(te_allocateTBEForEviction, "te", desc="allocate TBE Entry") {
    // this is for the victimized PF entry
    check_allocate(TBEs);
    TBEs.allocate(address);
    set_tbe(TBEs.lookup(address));
      tbe.writeMask.clear();
      tbe.wtData := false;
      tbe.atomicData := false;
      tbe.Dirty := false;
      tbe.NumPendingAcks := 0;
  }



  action(sf_setForwardReqTime, "sf", desc="...") {
    tbe.ForwardRequestTime := curCycle();
  }

  action(dl_deallocateL3, "dl", desc="deallocate the L3 block") {
    L3CacheMemory.deallocate(address);
  }

  action(pd_popDmaRequestQueue, "pd", desc="Pop DMA request queue") {
    dmaRequestQueue_in.dequeue(clockEdge());
  }

  action(p_popRequestQueue, "p", desc="pop request queue") {
    requestNetwork_in.dequeue(clockEdge());
  }

  action(pr_popResponseQueue, "pr", desc="pop response queue") {
    responseNetwork_in.dequeue(clockEdge());
  }

  action(pm_popMemQueue, "pm", desc="pop mem queue") {
    dequeueMemRespQueue();
  }

  action(pt_popTriggerQueue, "pt", desc="pop trigger queue") {
    triggerQueue_in.dequeue(clockEdge());
  }

  action(rt_recycleTriggerQueue, "rt", desc="recycle trigger queue") {
    if(earlyDataToCore) {
      triggerQueue_in.recycle(clockEdge(), cyclesToTicks(recycle_latency));
    }
  }

  action(ptl_popTriggerQueue, "ptl", desc="pop L3 trigger queue") {
    L3TriggerQueue_in.dequeue(clockEdge());
  }

  action(pu_popUnblockQueue, "pu", desc="pop unblock queue") {
    unblockNetwork_in.dequeue(clockEdge());
  }

  action(zz_recycleRequestQueue, "zz", desc="recycle request queue") {
    requestNetwork_in.recycle(clockEdge(), cyclesToTicks(recycle_latency));
  }

  action(yy_recycleResponseQueue, "yy", desc="recycle response queue") {
    responseNetwork_in.recycle(clockEdge(), cyclesToTicks(recycle_latency));
  }

  action(std_stallAndWaitDemandReq, "std", desc="Stall and wait on the deamnd address") {
    Addr demandAddr := tbe.demandAddress;
    stall_and_wait(requestNetwork_in, demandAddr);
  }

  action(st_stallAndWaitRequest, "st", desc="Stall and wait on the address") {
    stall_and_wait(requestNetwork_in, address);
  }

  action(sd_stallAndWaitRequest, "sd", desc="Stall and wait on the address") {
    stall_and_wait(dmaRequestQueue_in, address);
  }

  action(wad_wakeUpDependents, "wad", desc="Wake up any requests waiting for this address") {
    wakeUpBuffers(address);
  }

  action(wa_wakeUpAllDependents, "wa", desc="Wake up any requests waiting for this region") {
    wakeUpAllBuffers();
  }

  action(wada_wakeUpAllDependentsAddr, "wada", desc="Wake up any requests waiting for this address") {
    DPRINTF(RubySlicc, "wada wakeup: 0x%x\n", address);
    wakeUpAllBuffers(address);
  }

  /*
    Currently z_stall is unused because it can lead to Protocol Stalls that
    eventually lead to deadlock.  Instead, it is recommended to use
    st_stallAndWaitRequest in combination with a wakeupBuffer call (e.g.,
    wada_wakeUpAllDependentsAddr) to put the pending requests to sleep instead of
    them causing head of line blocking -- wada_wakeUpAllDependentsAddr should wake
    the request up once the request preventing it from completing is done.
  */
  action(z_stall, "z", desc="...") {
  }

  // TRANSITIONS
  transition({BL, BDR_M, BDW_M, BS_M, BM_M, B_M, BP, BDR_PM, BDW_PM, BS_PM, BM_PM, B_PM, BDR_Pm, BDW_Pm, BS_Pm, BM_Pm, B_Pm, B_P, B}, {RdBlkS, RdBlkM, RdBlk, CtoD}) {
      st_stallAndWaitRequest;
  }

  // It may be possible to save multiple invalidations here!
  transition({BL, BS_M, BM_M, B_M, BP, BS_PM, BM_PM, B_PM, BS_Pm, BM_Pm, B_Pm, B_P, B}, {Atomic, WriteThrough}) {
      st_stallAndWaitRequest;
  }

  // The exit state is always going to be U, so wakeUpDependents logic should be covered in all the
  // transitions which are flowing into U.
  transition({BL, BDR_M, BDW_M, BS_M, BM_M, B_M, BP, BDR_PM, BDW_PM, BS_PM, BM_PM, B_PM, BDR_Pm, BDW_Pm, BS_Pm, BM_Pm, B_Pm, B}, {DmaRead,DmaWrite}){
    sd_stallAndWaitRequest;
  }

  // transitions from U
  transition(U, PF_Repl, B_P) {PFTagArrayRead, PFTagArrayWrite}{
    te_allocateTBEForEviction;
    apf_allocateProbeFilterEntry; 
      // gratuitous, will always be allocated since replacement was ordered
      // TODO check by removing this
    bp_backProbe;
    sm_setMRU; // TODO we don't need this, right?
    mpfe_markPFEntryForEviction;
    std_stallAndWaitDemandReq; // puts the demand request to wait
  }

  transition(U, DmaRead, BDR_PM) {L3TagArrayRead} {
    atd_allocateTBEforDMA;
    qdr_queueDmaRdReq;
    pr_profileL3HitMiss; //Must come after qdr_queueDmaRdReq
    scd_probeShrCoreDataForDma;
    pd_popDmaRequestQueue;
  }

  transition(U, {RdBlkS}, BS_PM) {L3TagArrayRead} {
    t_allocateTBE;
    apf_allocateProbeFilterEntry;
    l_queueMemRdReq;
    pr_profileL3HitMiss; //Must come after l_queueMemRdReq
    sc_probeShrCoreData;
    sm_setMRU;
    upf_updateProbeFilter;
    p_popRequestQueue;
  }

  // TODO: Need to apply probeFilter changes in DMA too!!
  transition(U, DmaWrite, BDW_PM) {L3TagArrayRead} {
    atd_allocateTBEforDMA;
    qdr_queueDmaRdReq;
    pr_profileL3HitMiss; //Must come after qdr_queueDmaRdReq
    icd_probeInvCoreDataForDMA;
    pd_popDmaRequestQueue;
  }

  transition(U, WriteThrough, BM_PM) {L3TagArrayRead, L3TagArrayWrite} {
    t_allocateTBE;
    apf_allocateProbeFilterEntry;
    w_sendResponseWBAck;
    l_queueMemRdReq;
    pr_profileL3HitMiss; //Must come after l_queueMemRdReq
    dc_probeInvCoreData;
    sm_setMRU;
    upf_updateProbeFilter;
    p_popRequestQueue;
  }

  transition(U, Atomic, BM_PM) {L3TagArrayRead, L3TagArrayWrite} {
    t_allocateTBE;
    apf_allocateProbeFilterEntry;
    l_queueMemRdReq;
    pr_profileL3HitMiss; //Must come after l_queueMemRdReq
    dc_probeInvCoreData;
    sm_setMRU;
    upf_updateProbeFilter;
    p_popRequestQueue;
  }

  transition(U, {RdBlkM}, BM_PM) {L3TagArrayRead} {
    t_allocateTBE;
    apf_allocateProbeFilterEntry;
    l_queueMemRdReq;
    pr_profileL3HitMiss; //Must come after l_queueMemRdReq
    dc_probeInvCoreData;
    sm_setMRU;
    upf_updateProbeFilter;
    p_popRequestQueue;
  }

  transition(U, RdBlk, B_PM) {L3TagArrayRead}{
    t_allocateTBE;
    apf_allocateProbeFilterEntry;
    l_queueMemRdReq;
    pr_profileL3HitMiss; //Must come after l_queueMemRdReq
    sc_probeShrCoreData;
    sm_setMRU;
    upf_updateProbeFilter;
    p_popRequestQueue;
  }

  // CtoD is not a recognized event; so this is dummy for now
  transition(U, CtoD, BP) {L3TagArrayRead} {
    t_allocateTBE;
    apf_allocateProbeFilterEntry;
    ic_probeInvCore;
    sm_setMRU;
    upf_updateProbeFilter;
    p_popRequestQueue;
  }

  transition(U, VicDirty, BL) {L3TagArrayRead} {
    t_allocateTBE;
    w_sendResponseWBAck;
    rmcd_removeSharerConditional;
    p_popRequestQueue;
  }

  transition(U, VicClean, BL) {L3TagArrayRead} {
    t_allocateTBE;
    w_sendResponseWBAck;
    rmcd_removeSharerConditional;
    p_popRequestQueue;
  }

  transition(BL, {VicDirty, VicClean}) {
    zz_recycleRequestQueue;
  }

  transition(BL, CPUData, U) {L3TagArrayWrite, L3DataArrayWrite} {
    d_writeDataToMemory;
    al_allocateL3Block;
    pr_profileL3HitMiss; //Must come after al_allocateL3Block and before dt_deallocateTBE
    wada_wakeUpAllDependentsAddr;
    dt_deallocateTBE;
    pr_popResponseQueue;
  }

  transition(BL, StaleWB, U) {L3TagArrayWrite} {
    dt_deallocateTBE;
    wada_wakeUpAllDependentsAddr;
    pr_popResponseQueue;
  }

  transition({B, BDR_M, BDW_M, BS_M, BM_M, B_M, BP, BDR_PM, BDW_PM, BS_PM, BM_PM, B_PM, BDR_Pm, BDW_Pm, BS_Pm, BM_Pm, B_Pm, B_P}, {VicDirty, VicClean}) {
    // For B_P, if a line has requested VicClean/Dirty, adn tehn receives a backProbe (prbInvData)
    //       What happens?
    //       Seems like line will still invalidate, and send data and immediately send its original request
    //       because of which it needed victimizing in the first place
    st_stallAndWaitRequest;
  }

  /*  Sometimes, there is the possibility of getting an UnblockWriteThrough before ProbeAcksComplete
   *  because now we might be early-responding. And so, for those cases, when we encounter a premature
   *  UnblockWriteThrough, we can't just stall -- we'd need to recycle it. So the ProbeAcksComplete
   *  gets visibility.
   */
  transition({BM_Pm, BM_PM, BM_M}, UnblockWriteThrough) {
    rt_recycleTriggerQueue;
  }

  // there is now a new possibility of core unblocking before all probes/memory is read
  transition({BM_M, BM_PM, BM_Pm, BS_M, BS_PM, BS_Pm, B_M, B_PM, B_Pm}, CoreUnblock) {
    z_stall;
  }

  transition({BL, BS_M, BM_M, B_M, BP, BS_PM, BM_PM, B_PM, BS_Pm, BM_Pm, B_Pm, B_P, B}, PF_Repl) {
    // For B_P, since the PF victim is already being cleared out
    //          no need to stall since that'll lead to two (or more) PF replacements
    //          no need to recycle either (same)
    //          infact it should be completely disregarded (how to do that?)
    // TODO: One way would be to skip victimizing another line by having a exit-door at each event -- if the cacheAvail has somehow become true, skip probes
    zz_recycleRequestQueue;
  }

  transition({U, BL, BDR_M, BDW_M, BS_M, BM_M, B_M, BP, BDR_PM, BDW_PM, BS_PM, BM_PM, B_PM, BDR_Pm, BDW_Pm, BS_Pm, BM_Pm, B_Pm, B_P, B}, WBAck) {
    pm_popMemQueue;
  }

  transition({U, BL, BDR_M, BDW_M, BS_M, BM_M, B_M, BP, BDR_PM, BDW_PM, BS_PM, BM_PM, B_PM, BDR_Pm, BDW_Pm, BS_Pm, BM_Pm, B_Pm, B_P, B}, StaleVicDirty) {
    rv_removeVicDirtyIgnore;
    w_sendResponseWBAck;
    p_popRequestQueue;
  }

  transition({B}, CoreUnblock, U) {
    wada_wakeUpAllDependentsAddr;
    pu_popUnblockQueue;
  }

  transition(B, UnblockWriteThrough, U) {
    wada_wakeUpAllDependentsAddr;
    pt_popTriggerQueue;
  }

  transition(BDR_PM, MemData, BDR_Pm) {
    mt_writeMemDataToTBE;
    pm_popMemQueue;
  }

  transition(BDW_PM, MemData, BDW_Pm) {
    mt_writeMemDataToTBE;
    pm_popMemQueue;
  }

  transition(BS_PM, MemData, BS_Pm) {} {
    mt_writeMemDataToTBE;
    pm_popMemQueue;
  }

  transition(BM_PM, MemData, BM_Pm){} {
    mt_writeMemDataToTBE;
    pm_popMemQueue;
  }

  transition(B_PM, MemData, B_Pm){} {
    mt_writeMemDataToTBE;
    pm_popMemQueue;
  }

  transition(BDR_PM, L3Hit, BDR_Pm) {
    ptl_popTriggerQueue;
  }

  transition(BDW_PM, L3Hit, BDW_Pm) {
    ptl_popTriggerQueue;
  }

  transition(BS_PM, L3Hit, BS_Pm) {} {
    ptl_popTriggerQueue;
  }

  transition(BM_PM, L3Hit, BM_Pm) {} {
    ptl_popTriggerQueue;
  }

  transition(B_PM, L3Hit, B_Pm) {} {
    ptl_popTriggerQueue;
  }

  transition(BDR_M, MemData, U) {
    mt_writeMemDataToTBE;
    dd_sendResponseDmaData;
    wada_wakeUpAllDependentsAddr;
    dt_deallocateTBE;
    pm_popMemQueue;
  }

  transition(BDW_M, MemData, U) {
    mt_writeMemDataToTBE;
    wd_writeBackData;
    da_sendResponseDmaAck;
    wada_wakeUpAllDependentsAddr;
    dt_deallocateTBE;
    pm_popMemQueue;
  }

  transition(BS_M, MemData, B){L3TagArrayWrite, L3DataArrayWrite} {
    mt_writeMemDataToTBE;
    wd_writeBackData;
    s_sendResponseS;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    pm_popMemQueue;
  }

  transition(BM_M, MemData, B){L3TagArrayWrite, L3DataArrayWrite} {
    mt_writeMemDataToTBE;
    wd_writeBackData;
    m_sendResponseM;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    pm_popMemQueue;
  }

  transition(B_M, MemData, B){L3TagArrayWrite, L3DataArrayWrite} {
    mt_writeMemDataToTBE;
    wd_writeBackData;
    es_sendResponseES;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    pm_popMemQueue;
  }

  transition(BS_M, L3Hit, B) {L3TagArrayWrite, L3DataArrayWrite} {
    wd_writeBackData;
    s_sendResponseS;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    ptl_popTriggerQueue;
  }

  transition(BM_M, L3Hit, B) {L3DataArrayWrite, L3TagArrayWrite} {
    wd_writeBackData;
    m_sendResponseM;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    ptl_popTriggerQueue;
  }

  transition(B_M, L3Hit, B) {L3DataArrayWrite, L3TagArrayWrite} {
    wd_writeBackData;
    es_sendResponseES;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    ptl_popTriggerQueue;
  }

  transition({BDR_PM, BDW_PM, BS_PM, BM_PM, B_PM, BDR_Pm, BDW_Pm, BS_Pm, BM_Pm, B_Pm, B_P, BP}, CPUPrbResp) {
    y_writeProbeDataToTBE;
    edc_earlyDataToCore; // NA for B_P, but can be applied TODO
    x_decrementAcks;
    o_checkForCompletion;
    pr_popResponseQueue;
  }

  transition(BDR_PM, ProbeAcksComplete, BDR_M) {
    pt_popTriggerQueue;
  }

  transition(BDW_PM, ProbeAcksComplete, BDW_M) {
    pt_popTriggerQueue;
  }

  transition(BS_PM, ProbeAcksComplete, BS_M) {} {
    cupf_conditionalUpdatePF;
    sf_setForwardReqTime;
    pt_popTriggerQueue;
  }

  transition(BM_PM, ProbeAcksComplete, BM_M) {} {
    sf_setForwardReqTime;
    pt_popTriggerQueue;
  }

  transition(B_PM, ProbeAcksComplete, B_M){} {
    cupf_conditionalUpdatePF;
    sf_setForwardReqTime;
    pt_popTriggerQueue;
  }

  transition(BDR_Pm, ProbeAcksComplete, U) {
    dd_sendResponseDmaData;
    // Check for pending requests from the core we put to sleep while waiting
    // for a response
    wada_wakeUpAllDependentsAddr;
    dt_deallocateTBE;
    pt_popTriggerQueue;
  }

  transition(BDW_Pm, ProbeAcksComplete, U) {
    wd_writeBackData;
    da_sendResponseDmaAck;
    // Check for pending requests from the core we put to sleep while waiting
    // for a response
    wada_wakeUpAllDependentsAddr;
    dt_deallocateTBE;
    pt_popTriggerQueue;
  }

  transition(BS_Pm, ProbeAcksComplete, B){L3DataArrayWrite, L3TagArrayWrite} {
    cupf_conditionalUpdatePF;
    sf_setForwardReqTime;
    wd_writeBackData;
    s_sendResponseS;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    pt_popTriggerQueue;
  }

  transition(BM_Pm, ProbeAcksComplete, B){L3DataArrayWrite, L3TagArrayWrite} {
    sf_setForwardReqTime;
    wd_writeBackData;
    m_sendResponseM;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    pt_popTriggerQueue;
  }

  transition(B_Pm, ProbeAcksComplete, B){L3DataArrayWrite, L3TagArrayWrite} {
    cupf_conditionalUpdatePF;
    sf_setForwardReqTime;
    wd_writeBackData;
    es_sendResponseES;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    pt_popTriggerQueue;
  }

  // transition depends on whether B_P is reachable; 
  //  -- yes if ownerTrackign is enabled
  transition(B_P, ProbeAcksComplete, U) {
    wd_writeBackData;
    alwt_allocateL3BlockOnWT;
    we_wakeUpEvictionDependents;
    dpf_deallocateProbeFilter; // basically state I
    dt_deallocateTBE;
    pt_popTriggerQueue;
  }
  transition(BP, ProbeAcksComplete, B){L3TagArrayWrite, L3TagArrayWrite} {
    sf_setForwardReqTime;
    wd_writeBackData;
    c_sendResponseCtoD;
    alwt_allocateL3BlockOnWT;
    dt_deallocateTBE;
    pt_popTriggerQueue;
  }

 transition(U, Flush, F) {L3TagArrayRead, L3TagArrayWrite} {
    t_allocateTBE;
    f_writeFlushDataToMemory;
    w_sendResponseWBAck;
    p_popRequestQueue;
 }

 transition(F, WBAck, U) {
    pm_popMemQueue;
    dt_deallocateTBE;
 }

}
